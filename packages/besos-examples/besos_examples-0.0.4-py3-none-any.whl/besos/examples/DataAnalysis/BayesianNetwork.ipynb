{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041159,
     "end_time": "2019-10-28T22:48:07.872486",
     "exception": false,
     "start_time": "2019-10-28T22:48:07.831327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bayesian Network\n",
    "* * *\n",
    "### Objective\n",
    "In this Jupyter notebook, we build a stochastic generator of hourly electricity use in a residential building from measured data.\n",
    "This model can be used to generate many synthetic profiles of a given length as required by a Monte Carlo analysis.\n",
    "In this way a sensitivity analysis or robust optimization of Energy Hub outputs can be performed.\n",
    "\n",
    "The data used comes from [The Hourly Usage of Energy Dataset for Buildings in British Columbia](https://summit.sfu.ca/item/18163), which is available for free for non-commercial educational purposes.\n",
    "\n",
    "This notebook is organized as follows:\n",
    "- [Introduction](#Introduction) gives an overview of the approach and model formulation.\n",
    "- [Data loading and pre-processing](#Data-loading-and-pre-processing) performs the required initial actions on the measured data to obtain a proper data set for the model fitting.\n",
    "- [Conditional probability function fitting](#Conditional-probability-function-fitting) computes $P(U_t=u|M_t, H_t, W_t)$ from the measured data.\n",
    "- [Yearly profile generation](#Yearly-profile-generation) shows how using the model to generate profiles.\n",
    "- [Performance check](#Performance-check) proposes different tools to assess the similarities between synthetic and measured profiles.\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.619878,
     "end_time": "2019-10-28T22:48:08.510744",
     "exception": false,
     "start_time": "2019-10-28T22:48:07.890866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pk\n",
    "\n",
    "import chaospy as cp  # Uncertainty quantification using polynomial chaos expansions and Monte Carlo methods, and tools and classes to play with distributions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import dates as mdates, pylab as pl, pyplot as plt\n",
    "from scipy import fftpack as fft  # Fourier transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011555,
     "end_time": "2019-10-28T22:48:08.536405",
     "exception": false,
     "start_time": "2019-10-28T22:48:08.524850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "Buildings, renewable energy generation, storage technologies and associated energy systems all pose complex, interacting design and operational challenges. These are characterized by the underlying variation in boundary conditions, primarily the fluctuations in climate that affect solar and wind availability and thermal loads, but also the occupancy and use patterns of buildings and rooms. This work proposes a simple way to apply mathematical stochastic modelling techniques to these problems.\n",
    "\n",
    "The idea is to build a stochastic profile generator from measured data to obtain synthetic hourly electricity demand of a residential building over a year. This generator will be used as input of [Energy Hub models](../EnergyHub/Overview.ipynb) and will be the first piece of a stochastic formulation of the Energy Hub problem.\n",
    "\n",
    "This notebook is a first attempt to build such a generator by means of a simple [Bayesian network](https://en.wikipedia.org/wiki/Bayesian_network).\n",
    "The proposed Bayesian network to predict the electricity use for the hour t is the following:<br>\n",
    "<img src=\"attachment:image.png\" alt=\"drawing\" width=\"200\" float=\"left\"/>\n",
    "- U: electricity use (kW)\n",
    "- M: month in the year [1:12]\n",
    "- H: hour of the day [0:23]\n",
    "- W: weekday/weekend (0 or 1)\n",
    "\n",
    "In other words, we assume that the electricity use of hour t is only dependant on the month of the year, the hour of the day and wether the day is a weekday or a weekend.\n",
    "The electricity use is a stochastic variable and so we can define its probability as:\n",
    "$$P(U_t=u)=P(U_t=u|M_t, H_t, W_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012677,
     "end_time": "2019-10-28T22:48:08.562715",
     "exception": false,
     "start_time": "2019-10-28T22:48:08.550038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data loading and pre-processing\n",
    "## Loading\n",
    "The data set of the building is saved as csv file whose name contains its id in HUE study.\n",
    "It is an appartment in Metro Vancouver area, equipped with an electric in-floor radiant heating.\n",
    "The DataFrame is formatted to have the date as the index and one column which contains the total electricity demand for the past hour in kWh. There are also some missing values (0.4% of all the values) which are removed from the data set.\n",
    "The time step is therefore one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041904,
     "end_time": "2019-10-28T22:48:08.616344",
     "exception": false,
     "start_time": "2019-10-28T22:48:08.574440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pathData = \"\"  # relative path to data\n",
    "dataid = 22  # Building id\n",
    "\n",
    "# Load data for the building\n",
    "df = pd.read_csv(os.path.join(pathData, \"Residential_\" + str(dataid) + \".csv\"))\n",
    "\n",
    "# Format the index of the dataframe so it is the date\n",
    "df.index = pd.to_datetime(df[\"date\"])\n",
    "df.index = df.index + pd.to_timedelta(df.hour, unit=\"h\")\n",
    "building = df.drop(columns=[\"date\", \"hour\"])\n",
    "\n",
    "# Rename the column\n",
    "building = building.rename(columns={\"energy_kWh\": \"use\"})\n",
    "\n",
    "# Remove the missing values (Nans)\n",
    "building = building.dropna()\n",
    "\n",
    "# Show the first lines of the DataFrame\n",
    "building.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014817,
     "end_time": "2019-10-28T22:48:08.644832",
     "exception": false,
     "start_time": "2019-10-28T22:48:08.630015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pre-processing\n",
    "Three columns are added to the DataFrame to facilitate model fitting:\n",
    "- *month*: an integer between 1 and 12 corresponding to the current month\n",
    "- *hourOfDay*: an integer between 0 and 23 corresponding to the current hour of the day\n",
    "- *isWeekday*: whether the current day is a weekday ($isWeekday=1$) or not ($isWeekday=0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.03574,
     "end_time": "2019-10-28T22:48:08.694732",
     "exception": false,
     "start_time": "2019-10-28T22:48:08.658992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add a column for the month number\n",
    "month = building.index.month\n",
    "building[\"month\"] = month\n",
    "\n",
    "# Add a column for the hour in day number\n",
    "hourOfDay = building.index.hour\n",
    "building[\"hourOfDay\"] = hourOfDay\n",
    "\n",
    "# Add a column for the type of day (weekday=1, weekend=0)\n",
    "dayOfWeek = building.index.weekday\n",
    "isWeekday = np.zeros_like(dayOfWeek)\n",
    "isWeekday[dayOfWeek < 4] = 1\n",
    "building[\"isWeekday\"] = isWeekday\n",
    "\n",
    "# Show the first lines of the DataFrame\n",
    "building.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013386,
     "end_time": "2019-10-28T22:48:08.721874",
     "exception": false,
     "start_time": "2019-10-28T22:48:08.708488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conditional probability function fitting\n",
    "The idea here is to compute $P(U_t=u|M_t, H_t, W_t)$ from the measured data.\n",
    "Taking advantage of the fact that $M_t$, $H_t$ and $W_t$ are discrete and countable, it equates to learn a probability density function per triple $(M_t, H_t, W_t)$.\n",
    "We will therefore learn $12*24*2= \\color{blue}{576}$ distributions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.715352,
     "end_time": "2019-10-28T22:48:09.453543",
     "exception": false,
     "start_time": "2019-10-28T22:48:08.738191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the empty arrays\n",
    "distributions = np.empty([12, 24, 2], dtype=object)\n",
    "samplesNumber = np.zeros_like(distributions)\n",
    "\n",
    "# Compute the distributions\n",
    "for k, g in building.groupby(building.month):  # group by month and loop\n",
    "    for k1, g1 in g.groupby(g.hourOfDay):  # group by hour of day and loop\n",
    "        for k2, g2 in g1.groupby(g1.isWeekday):  # group by type of day and loop\n",
    "            # Fit one distribution for each month (k-1), each hour of day k1, each type of day k2\n",
    "            distributions[k - 1, k1, k2] = cp.SampleDist(\n",
    "                g2.use\n",
    "            )  # actual fitting function from chaospy\n",
    "            # Number of samples used for fitting each distribution\n",
    "            samplesNumber[k - 1, k1, k2] = len(g2)\n",
    "\n",
    "# Print some information about the number of samples used for the distributions\n",
    "print(\"Minimum number of samples for a distribution: \" + str(samplesNumber.min()))\n",
    "print(\"Maximum number of samples for a distribution: \" + str(samplesNumber.max()))\n",
    "\n",
    "# Save distributions\n",
    "with open(\"Models\\\\NaiveBayesianDistributions_\" + str(dataid), \"wb\") as fp:\n",
    "    pk.dump(distributions, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013173,
     "end_time": "2019-10-28T22:48:09.479721",
     "exception": false,
     "start_time": "2019-10-28T22:48:09.466548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It is interesting to first examine the proability density functions obtained.\n",
    "Two figures are generated:\n",
    "- one containing all 576 density functions in one plot,\n",
    "- one representing the monthly evolution of the density functions for each type of day and each hour of day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 4.967461,
     "end_time": "2019-10-28T22:48:14.462201",
     "exception": false,
     "start_time": "2019-10-28T22:48:09.494740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the x-axis values for the evaluation of the functions (electricity use)\n",
    "use = np.linspace(0, 6, 500)\n",
    "# Set the different colors for the months\n",
    "colors = pl.cm.jet(np.linspace(0, 1, 12))\n",
    "\n",
    "# Prepare figures and subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 4))  # a global plot\n",
    "fig1, ax1 = plt.subplots(\n",
    "    8, 6, figsize=(16, 10)\n",
    ")  # plots to compare month, hour of day and type of day\n",
    "# One set of axis per type of day\n",
    "ax1a = ax1[:, :3].reshape(24)\n",
    "ax1b = ax1[:, 3:].reshape(24)\n",
    "\n",
    "# For-loop to loop over the distributions array\n",
    "for i in np.arange(12):  # loop over months\n",
    "    p = 0\n",
    "    for j in np.arange(24):  # loop over hours of day\n",
    "        for k in np.arange(2):  # loop over types of day\n",
    "            ax.plot(use, distributions[i, j, k].pdf(use))\n",
    "        ax1a[p].plot(use, distributions[i, j, 1].pdf(use), label=i + 1, color=colors[i])\n",
    "        ax1b[p].plot(use, distributions[i, j, 0].pdf(use), label=i + 1, color=colors[i])\n",
    "        p = p + 1\n",
    "\n",
    "# Formatting the global plot\n",
    "ax.set_xlim([0, 6])\n",
    "ax.set_xlabel(\"Heating demand (kWh)\")\n",
    "ax.set_ylabel(\"Probability density distribution\")\n",
    "ax.legend()\n",
    "\n",
    "# Formatting the second figure\n",
    "# Formatting each subplot\n",
    "for p in np.arange(24):\n",
    "    ax1a[p].set_xlim([0, 6])\n",
    "    ax1a[p].set_ylim([0, 6])\n",
    "    ax1a[p].axis(\"off\")\n",
    "    ax1a[p].text(\n",
    "        2,\n",
    "        2,\n",
    "        str(p) + \":00\",\n",
    "        fontsize=12,\n",
    "        bbox={\"facecolor\": \"white\", \"alpha\": 0.5, \"pad\": 5},\n",
    "    )\n",
    "    ax1b[p].set_xlim([0, 6])\n",
    "    ax1b[p].set_ylim([0, 6])\n",
    "    ax1b[p].axis(\"off\")\n",
    "    ax1b[p].text(\n",
    "        2,\n",
    "        2,\n",
    "        str(p) + \":00\",\n",
    "        fontsize=12,\n",
    "        bbox={\"facecolor\": \"white\", \"alpha\": 0.5, \"pad\": 5},\n",
    "    )\n",
    "# Add a line between subplots for weekday and weekend\n",
    "line = plt.Line2D([0.51, 0.51], [0.1, 0.9], transform=fig1.transFigure, color=\"black\")\n",
    "fig1.add_artist(line)\n",
    "# Add a global legend\n",
    "handles, labels = ax1a[0].get_legend_handles_labels()\n",
    "fig1.legend(handles, labels, loc=\"center right\")\n",
    "# Add title and subtitles\n",
    "fig1.suptitle(\"Probability density functions per month\", fontsize=16)\n",
    "ax1a[0].text(0, 7, \"Weekday\", fontsize=15)\n",
    "ax1b[2].text(6, 7, \"Weekend\", fontsize=15, horizontalalignment=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015302,
     "end_time": "2019-10-28T22:48:14.493133",
     "exception": false,
     "start_time": "2019-10-28T22:48:14.477831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font color=green>Comments:</font>\n",
    "- Globally we observe a high probability of an electricity demand of around 0.2 kWh. There is also a rapid decrease of the density functions below 0.1 kWh which seems to indicate that this is the base load of this building. We can also observe a second peak around 0.4 kWh which can correspond to the operation of the electric in-floor radiant heating.\n",
    "- Weekends are usually characterized by a slightly higher diversity of the demand during the morning (less sharped peaks) and less diversity during end of afternoon and evening. The main differences between weekday and weekend occur during the working hours and evenings (7:00-23:00).\n",
    "- Vancouver (British Columbia, Canada) is characterized by a mild winter and a mild summer. It seems that this house does not have a cooling device as electricity demand is generally higher in winter (because of the in-floor heating). Indeed, we can observe a spread and a shift of the peak towards higher values of electricity demand for winter months. This tendency is more pronounced during the end of the night, indicating that when people are awake the other appliance energy demand is high compared to the one of the heating device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017709,
     "end_time": "2019-10-28T22:48:14.527053",
     "exception": false,
     "start_time": "2019-10-28T22:48:14.509344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Yearly profile generation\n",
    "This section shows how to use the model to generate hourly electricity demand for one year.\n",
    "The final profile will be stochastic (i.e. different for each run).\n",
    "The general procedure is as follows:\n",
    "\n",
    "- Create a DataFrame with hours for one year as the index and corresponding *month*, *hourOfDay* and *isWeekday* values.\n",
    "- For each row (i.e. each hour), generate an electricity demand value from the corresponding proability density function.\n",
    "\n",
    "**Remarks:**\n",
    "- The proposed procedure can generate several profiles at the same time (here 10), to enable sampling of the distributions.\n",
    "The sampling method can be changed through the parameters of the *sample* function (see [chaospy documentation](https://chaospy.readthedocs.io/en/development/distributions/sampling.html) for further information).\n",
    "- An array is first populated before populating the DataFrame for computationally efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.017803,
     "end_time": "2019-10-28T22:48:17.561218",
     "exception": false,
     "start_time": "2019-10-28T22:48:14.543415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Number of profiles to generate\n",
    "nb_profile = 10\n",
    "##\n",
    "\n",
    "## Create the base dataFrame\n",
    "profiles = pd.DataFrame(index=pd.date_range(start=\"1/1/2019\", end=\"1/1/2020\", freq=\"H\"))\n",
    "# Add a column for the month number\n",
    "month = profiles.index.month\n",
    "profiles[\"month\"] = month\n",
    "# Add a column for the hour in day number\n",
    "hourOfDay = profiles.index.hour\n",
    "profiles[\"hourOfDay\"] = hourOfDay\n",
    "# Add a column for the type of day (weekday=1, weekend=0)\n",
    "dayOfWeek = profiles.index.weekday\n",
    "isWeekday = np.zeros_like(dayOfWeek)\n",
    "isWeekday[dayOfWeek < 4] = 1\n",
    "profiles[\"isWeekday\"] = isWeekday\n",
    "\n",
    "## Populate with Bayesian network model\n",
    "# Create data in a table\n",
    "tab_profiles = []\n",
    "for index, time in profiles.iterrows():  # loop over the rows of the DataFrame\n",
    "    # add electricity demand for all the profiles for this time, using samplig method\n",
    "    tab_profiles.append(\n",
    "        distributions[time.month - 1, time.hourOfDay, time.isWeekday].sample(nb_profile)\n",
    "    )\n",
    "tab_profiles = np.array(tab_profiles)\n",
    "\n",
    "## Create new columns\n",
    "for p in np.arange(nb_profile):\n",
    "    profiles[\"use\" + str(p)] = tab_profiles[:, p]\n",
    "profiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017184,
     "end_time": "2019-10-28T22:48:17.595186",
     "exception": false,
     "start_time": "2019-10-28T22:48:17.578002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's have a quick look to the resulting profiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.71418,
     "end_time": "2019-10-28T22:48:18.326243",
     "exception": false,
     "start_time": "2019-10-28T22:48:17.612063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 4))\n",
    "# Plot all the synthetic profiles on the same plot\n",
    "for p in np.arange(nb_profile):\n",
    "    ax.plot(profiles.index, profiles[\"use\" + str(p)])\n",
    "# Format the dates for x-axis\n",
    "myFmt = mdates.DateFormatter(\"%m-%d\")\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "# Add labels\n",
    "ax.set_ylabel(\"Electricity demand (kWh)\")\n",
    "ax.set_xlabel(\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02079,
     "end_time": "2019-10-28T22:48:18.366268",
     "exception": false,
     "start_time": "2019-10-28T22:48:18.345478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can see that the profiles are all different, but seem to have retained the underlying trends seen in the original data.\n",
    "We will get a better idea of whether the synthetic profiles retain the correct statistical properties in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017093,
     "end_time": "2019-10-28T22:48:18.402846",
     "exception": false,
     "start_time": "2019-10-28T22:48:18.385753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Performance check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017196,
     "end_time": "2019-10-28T22:48:18.437807",
     "exception": false,
     "start_time": "2019-10-28T22:48:18.420611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Assessing the performance of a stochastic model can be tricky.\n",
    "Here we propose several basic ways, but better metrics can be found.\n",
    "These approaches are based on the comparison of several features:\n",
    "- distribution of total annual demands,\n",
    "- evolution and distribution of daily demands,\n",
    "- autocorrelation for a given time series,\n",
    "- discrete Fourier transform of the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01733,
     "end_time": "2019-10-28T22:48:18.473219",
     "exception": false,
     "start_time": "2019-10-28T22:48:18.455889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Total annual demands\n",
    "Comparing the distribution of the annual demands enables us to check that the model reproduces the overall annual variations over the 3 years present in the  measured data.\n",
    "Yearly demands are computed and plotted on a 1-D graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.038463,
     "end_time": "2019-10-28T22:48:18.531778",
     "exception": false,
     "start_time": "2019-10-28T22:48:18.493315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sum the synthetic demand over the year for each profile (=each column)\n",
    "synthYearUse = profiles.iloc[:, 3:].sum()\n",
    "\n",
    "# We need a complete data set to compute the different checks.\n",
    "# Interpolation for missing values\n",
    "buildingComplete = building.copy()\n",
    "buildingComplete = buildingComplete.resample(\"1H\").sum()\n",
    "buildingComplete.interpolate()\n",
    "# Compute the measured yearly demand for each year\n",
    "measYearUse = []\n",
    "for k, g in buildingComplete.groupby(\n",
    "    buildingComplete.index.year\n",
    "):  # group by year and loop\n",
    "    if len(g) >= 8760:  # only complete years\n",
    "        # sum the demand over the year\n",
    "        measYearUse.append(g.use.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.142347,
     "end_time": "2019-10-28T22:48:18.691836",
     "exception": false,
     "start_time": "2019-10-28T22:48:18.549489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "# Create the figure\n",
    "fig1, ax1 = plt.subplots(1, 1, figsize=(5, 4))\n",
    "# Plot the yearly demands as events (= 1D graph)\n",
    "ax1.eventplot(\n",
    "    measYearUse, orientation=\"vertical\", lineoffsets=0, colors=\"b\", label=\"measured\"\n",
    ")\n",
    "ax1.eventplot(\n",
    "    synthYearUse, orientation=\"vertical\", lineoffsets=0, colors=\"r\", label=\"synthetic\"\n",
    ")\n",
    "# Format the figure\n",
    "ax1.set_xlim([-10, 10])\n",
    "ax1.axes.get_xaxis().set_visible(False)\n",
    "ax1.set_ylabel(\"Yearly demand\")\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022,
     "end_time": "2019-10-28T22:48:18.733737",
     "exception": false,
     "start_time": "2019-10-28T22:48:18.711737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font color=green>Comments:</font>\n",
    "- Unfortunately the data set contains only one whole year so it is hard to derive a lot of useful information. We can only say that the measured yearly demand is on the same order of magnitude as the synthetic ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019826,
     "end_time": "2019-10-28T22:48:18.773412",
     "exception": false,
     "start_time": "2019-10-28T22:48:18.753586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Daily demand\n",
    "Increasing the granularity, we can compare the daily demands and their evolution over the year.\n",
    "Daily demands are computed and plotted against day of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.805019,
     "end_time": "2019-10-28T22:48:19.596798",
     "exception": false,
     "start_time": "2019-10-28T22:48:18.791779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the measured daily demands\n",
    "# Create list of days of year\n",
    "synthDayOfYear = np.arange(366)\n",
    "# Create empty list\n",
    "synthDayUse = []\n",
    "for k, g in profiles.groupby(profiles.index.date):  # group by date and loop\n",
    "    # Sum demands over the day\n",
    "    # The final shape of synthDayUse is (366, nb_profile)\n",
    "    synthDayUse.append(g.sum()[3:])\n",
    "\n",
    "# Compute the measured daily demands\n",
    "# Create empty lists\n",
    "measDayOfYear = []\n",
    "measDayUse = []\n",
    "for k, g in buildingComplete.groupby(\n",
    "    buildingComplete.index.date\n",
    "):  # group by date and loop\n",
    "    if len(g) >= 24:  # only complete days\n",
    "        # Store day of year for plotting\n",
    "        measDayOfYear.append(g.index[0].dayofyear)\n",
    "        # Sum demands over the day\n",
    "        # The final shape of measDayUse is (, len(building))\n",
    "        measDayUse.append(g.use.sum())\n",
    "\n",
    "# Plot the results\n",
    "# Create the figure\n",
    "fig1, ax1 = plt.subplots(1, 1, figsize=(20, 4))\n",
    "# Plot the daily demands as scatter plot\n",
    "line1 = ax1.scatter(measDayOfYear, measDayUse)\n",
    "lines2 = ax1.plot(synthDayOfYear, synthDayUse, \"r+\")\n",
    "# Format the figure\n",
    "ax1.set_xlabel(\"Day of the year\")\n",
    "ax1.set_ylabel(\"Daily demand\")\n",
    "ax1.legend([line1, lines2[0]], [\"measured\", \"synthetic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022444,
     "end_time": "2019-10-28T22:48:19.640770",
     "exception": false,
     "start_time": "2019-10-28T22:48:19.618326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font color=green>Comments:</font>\n",
    "- The trend over the year is very well reproduced.\n",
    "- The variability of synthetic data is a bit lower. The model seems to be too conservative. This could also come from the sampling method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02065,
     "end_time": "2019-10-28T22:48:19.682798",
     "exception": false,
     "start_time": "2019-10-28T22:48:19.662148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Autocorrelation\n",
    "Electricity demand time series can have high autocorrelation due to repeating patterns.\n",
    "Autocorrelation plots highlight the time constants of the main patterns.\n",
    "As we also search for yearly autocorrelation (because of the yearly trend seen above), it is necessary to create an artificial data set over several years from the synthetic profiles. This is done by concatenating all the profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.129882,
     "end_time": "2019-10-28T22:48:21.831790",
     "exception": false,
     "start_time": "2019-10-28T22:48:19.701908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the figure\n",
    "fig, ax1 = plt.subplots(2, 1, figsize=(15, 3 * 3))\n",
    "# Global parameters for the two subplots\n",
    "xlim = [-9000, 9000]\n",
    "barwidth = 25\n",
    "\n",
    "## Create artificially a dataset over several years\n",
    "## by concatenating all the profiles\n",
    "allProfiles = []\n",
    "for p in np.arange(nb_profile):\n",
    "    allProfiles = np.append(profiles[\"use\" + str(p)].values, allProfiles)\n",
    "\n",
    "## Plot autocorrelation for synthetic profile\n",
    "lags1 = ax1[0].acorr(allProfiles, maxlags=9000, color=\"grey\", zorder=0)\n",
    "# Emphasis the day before, the week before and the year before\n",
    "ax1[0].bar(\n",
    "    lags1[0][np.where(abs(lags1[0]) == 24)[0]],\n",
    "    lags1[1][np.where(abs(lags1[0]) == 24)[0]],\n",
    "    color=\"red\",\n",
    "    width=barwidth,\n",
    "    label=\"day before\",\n",
    ")\n",
    "ax1[0].bar(\n",
    "    lags1[0][np.where(abs(lags1[0]) == 7 * 24)[0]],\n",
    "    lags1[1][np.where(abs(lags1[0]) == 7 * 24)[0]],\n",
    "    color=\"green\",\n",
    "    width=barwidth,\n",
    "    label=\"week before\",\n",
    ")\n",
    "ax1[0].bar(\n",
    "    lags1[0][np.where(abs(lags1[0]) == 365 * 24)[0]],\n",
    "    lags1[1][np.where(abs(lags1[0]) == 365 * 24)[0]],\n",
    "    color=\"blue\",\n",
    "    width=barwidth,\n",
    "    label=\"year before\",\n",
    ")\n",
    "# Add labels to autocorrelation plot\n",
    "ax1[0].set_ylabel(\"Autocorr. of\\n synthetic profile\")\n",
    "ax1[0].set_xlabel(\"Lag (days)\")\n",
    "ax1[0].set_xlim(xlim)\n",
    "# Display the autocorrelation plot\n",
    "ax1[0].legend()\n",
    "\n",
    "## Plot autocorrelation for measured data\n",
    "lags2 = ax1[1].acorr(buildingComplete.use, maxlags=9000, color=\"grey\", zorder=0)\n",
    "# Emphasis the day before, the week before and the year before\n",
    "ax1[1].bar(\n",
    "    lags2[0][np.where(abs(lags2[0]) == 24)[0]],\n",
    "    lags2[1][np.where(abs(lags2[0]) == 24)[0]],\n",
    "    color=\"red\",\n",
    "    width=barwidth,\n",
    "    label=\"day before\",\n",
    ")\n",
    "ax1[1].bar(\n",
    "    lags2[0][np.where(abs(lags2[0]) == 7 * 24)[0]],\n",
    "    lags2[1][np.where(abs(lags2[0]) == 7 * 24)[0]],\n",
    "    color=\"green\",\n",
    "    width=barwidth,\n",
    "    label=\"week before\",\n",
    ")\n",
    "ax1[1].bar(\n",
    "    lags2[0][np.where(abs(lags2[0]) == 365 * 24)[0]],\n",
    "    lags2[1][np.where(abs(lags2[0]) == 365 * 24)[0]],\n",
    "    color=\"blue\",\n",
    "    width=barwidth,\n",
    "    label=\"year before\",\n",
    ")\n",
    "# Add labels to autocorrelation plot\n",
    "ax1[1].set_ylabel(\"Autocorr. of\\n measured data\")\n",
    "ax1[1].set_xlabel(\"Lag (days)\")\n",
    "ax1[1].set_xlim(xlim)\n",
    "# Display the autocorrelation plot\n",
    "ax1[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02398,
     "end_time": "2019-10-28T22:48:21.878026",
     "exception": false,
     "start_time": "2019-10-28T22:48:21.854046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Zooming in on these figures gives a better idea of short-term autocorrelations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.98327,
     "end_time": "2019-10-28T22:48:23.883928",
     "exception": false,
     "start_time": "2019-10-28T22:48:21.900658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the figure\n",
    "fig, ax1 = plt.subplots(2, 1, figsize=(15, 3 * 3))\n",
    "# Global parameters for the two subplots\n",
    "xlim = [-8 * 24, 8 * 24]\n",
    "barwidth = 1\n",
    "\n",
    "## Plot autocorrelation for synthetic profile\n",
    "lags1 = ax1[0].acorr(allProfiles, maxlags=8760, color=\"grey\", zorder=0)\n",
    "# Emphasis the day before, the week before and the year before\n",
    "ax1[0].bar(\n",
    "    lags1[0][np.where(abs(lags1[0]) == 24)[0]],\n",
    "    lags1[1][np.where(abs(lags1[0]) == 24)[0]],\n",
    "    color=\"red\",\n",
    "    width=barwidth,\n",
    "    label=\"day before\",\n",
    ")\n",
    "ax1[0].bar(\n",
    "    lags1[0][np.where(abs(lags1[0]) == 7 * 24)[0]],\n",
    "    lags1[1][np.where(abs(lags1[0]) == 7 * 24)[0]],\n",
    "    color=\"green\",\n",
    "    width=barwidth,\n",
    "    label=\"week before\",\n",
    ")\n",
    "ax1[0].bar(\n",
    "    lags1[0][np.where(abs(lags1[0]) == 365 * 24)[0]],\n",
    "    lags1[1][np.where(abs(lags1[0]) == 365 * 24)[0]],\n",
    "    color=\"blue\",\n",
    "    width=barwidth,\n",
    "    label=\"year before\",\n",
    ")\n",
    "# Add labels to autocorrelation plot\n",
    "ax1[0].set_ylabel(\"Autocorr. of\\n synthetic profile\")\n",
    "ax1[0].set_xlabel(\"Lag (days)\")\n",
    "ax1[0].set_xlim(xlim)\n",
    "# Display the autocorrelation plot\n",
    "ax1[0].legend()\n",
    "\n",
    "## Plot autocorrelation for measured data\n",
    "lags2 = ax1[1].acorr(buildingComplete.use, maxlags=9000, color=\"grey\", zorder=0)\n",
    "# Emphasis the day before, the week before and the year before\n",
    "ax1[1].bar(\n",
    "    lags2[0][np.where(abs(lags2[0]) == 24)[0]],\n",
    "    lags2[1][np.where(abs(lags2[0]) == 24)[0]],\n",
    "    color=\"red\",\n",
    "    width=barwidth,\n",
    "    label=\"day before\",\n",
    ")\n",
    "ax1[1].bar(\n",
    "    lags2[0][np.where(abs(lags2[0]) == 7 * 24)[0]],\n",
    "    lags2[1][np.where(abs(lags2[0]) == 7 * 24)[0]],\n",
    "    color=\"green\",\n",
    "    width=barwidth,\n",
    "    label=\"week before\",\n",
    ")\n",
    "ax1[1].bar(\n",
    "    lags2[0][np.where(abs(lags2[0]) == 365 * 24)[0]],\n",
    "    lags2[1][np.where(abs(lags2[0]) == 365 * 24)[0]],\n",
    "    color=\"blue\",\n",
    "    width=barwidth,\n",
    "    label=\"year before\",\n",
    ")\n",
    "# Add labels to autocorrelation plot\n",
    "ax1[1].set_ylabel(\"Autocorr. of\\n measured data\")\n",
    "ax1[1].set_xlabel(\"Lag (days)\")\n",
    "ax1[1].set_xlim(xlim)\n",
    "# Display the autocorrelation plot\n",
    "ax1[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023728,
     "end_time": "2019-10-28T22:48:23.930267",
     "exception": false,
     "start_time": "2019-10-28T22:48:23.906539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font color=green>Comments:</font>\n",
    "- The model works quite well according to these feature: the global pattern of the autocorrelation plot is well reproduced.\n",
    "- However autocorrelation from one year to another is overestimated by the model (beginning and end of the first graph).\n",
    "- The correlation with previous hour is on the contrary underestimated by the model (lines on both sides of the central one). This is expected as this model does not take account of the previous hour. A [Markov Chain model](https://en.wikipedia.org/wiki/Markov_chain) would give better results in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021917,
     "end_time": "2019-10-28T22:48:23.975450",
     "exception": false,
     "start_time": "2019-10-28T22:48:23.953533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Discrete Fourier transform\n",
    "The discrete Fourier transform decomposes a signal into its sine and cosine components.\n",
    "Each component is described by its frequency and amplitude.\n",
    "Looking at the frequencies which have the highest amplitudes is another way to find periodic patterns in a time series.\n",
    "\n",
    "The synthetic data over several years is used again here to perform the discrete Fourier transform.\n",
    "\n",
    "**Remark:** The second graph is again a zoomed in version of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.61724,
     "end_time": "2019-10-28T22:48:24.613797",
     "exception": false,
     "start_time": "2019-10-28T22:48:23.996557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the figure\n",
    "fig2, ax2 = plt.subplots(2, 1, figsize=(10, 6))\n",
    "\n",
    "## Synthetic profiles\n",
    "# Compute the FFT of the signal\n",
    "synthUse_fft = fft.fft(allProfiles)\n",
    "# Compute the power spectral density (PWD)\n",
    "synthUse_psd = np.abs(synthUse_fft) ** 2\n",
    "# Compute the frequencies of the PWDs\n",
    "timestep = 1 / (365 * 24)  # year based\n",
    "fftfreq = fft.fftfreq(len(synthUse_psd), timestep)\n",
    "i = fftfreq >= 0  # keep only the positive frequencies\n",
    "# Plot the results\n",
    "ax2[0].plot(fftfreq[i], synthUse_psd[i], label=\"synthetic\")\n",
    "ax2[1].plot(fftfreq[i], synthUse_psd[i], label=\"synthetic\")\n",
    "\n",
    "## Measurement data\n",
    "# Compute the FFT of the signal\n",
    "measUse_fft = fft.fft(buildingComplete.use.values)\n",
    "# Compute the power spectral density (PWD)\n",
    "measUse_psd = np.abs(measUse_fft) ** 2\n",
    "# Compute the frequencies of the PWDs\n",
    "timestep = 1 / (365 * 24)  # year based\n",
    "fftfreq = fft.fftfreq(len(measUse_psd), timestep)\n",
    "i = fftfreq >= 0  # keep only the positive frequencies\n",
    "# Plot the results\n",
    "ax2[0].plot(fftfreq[i], measUse_psd[i], label=\"measured\")\n",
    "ax2[1].plot(fftfreq[i], measUse_psd[i], label=\"measured\")\n",
    "\n",
    "# Format subplots\n",
    "ax2[0].set_xlim(-10, 400)\n",
    "ax2[0].set_ylim(0, 0.1e8)\n",
    "ax2[0].set_ylabel(\"PSD\")\n",
    "ax2[0].set_xlabel(\"Frequency (1/year)\")\n",
    "ax2[0].legend()\n",
    "ax2[1].set_xlim(0, 10)  # the second one is a zoom from the first one\n",
    "ax2[1].set_ylim(0, 0.1e8)\n",
    "ax2[1].set_ylabel(\"PSD\")\n",
    "ax2[1].set_xlabel(\"Frequency (1/year)\")\n",
    "ax2[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026104,
     "end_time": "2019-10-28T22:48:24.663997",
     "exception": false,
     "start_time": "2019-10-28T22:48:24.637893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Yearly trend corresponds to the frequency $f=1$.\n",
    "- The frequency $f=2$ is related to a bi-yearly, i.e. seasonal trend.\n",
    "- There is a peak at $f=52$ for weekly trend (accounted for by distinction between weekdays and weekends).\n",
    "- The daily pattern is clearly visible for $f=365$.\n",
    "\n",
    "<font color=green>Comments:</font>\n",
    "- It seems that the model reproduces *too well* the different trends, i.e. the corresponding amplitudes are more significant than they should be.\n",
    "Again this implies that the model is too conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.021992,
     "end_time": "2019-10-28T22:48:24.710363",
     "exception": false,
     "start_time": "2019-10-28T22:48:24.688371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
