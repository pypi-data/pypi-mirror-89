This file is auto-generated and should reflect the current state.

This is a list of supported PyTorch modules and functions.
See also [what is unsupported (on a higher level)](Unsupported.md).

* `torch.nn.parameter` (module) available: ✅
* `torch.nn.grad` (module) available: ❌
* `torch.nn.functional` (module) available: ✅
* `torch.nn.init` (module) available: ✅
* `torch.nn.common_types` (module) available: ✅
* `torch.nn.utils` (module) available: ✅
* `torch.nn.modules` (module) available: ✅
* `torch.nn.Module` (torch.nn.Module) available: ✅
* `torch.nn.Identity` (torch.nn.Module) available: ✅
* `torch.nn.Linear` (torch.nn.Module) available: ✅
* `torch.nn.Conv1d` (torch.nn.Module) available: ✅
* `torch.nn.Conv2d` (torch.nn.Module) available: ✅
* `torch.nn.Conv3d` (torch.nn.Module) available: ❌
* `torch.nn.ConvTranspose1d` (torch.nn.Module) available: ✅
* `torch.nn.ConvTranspose2d` (torch.nn.Module) available: ✅
* `torch.nn.ConvTranspose3d` (torch.nn.Module) available: ❌
* `torch.nn.Threshold` (torch.nn.Module) available: ❌
* `torch.nn.ReLU` (torch.nn.Module) available: ✅
* `torch.nn.Hardtanh` (torch.nn.Module) available: ❌
* `torch.nn.ReLU6` (torch.nn.Module) available: ❌
* `torch.nn.Sigmoid` (torch.nn.Module) available: ✅
* `torch.nn.Tanh` (torch.nn.Module) available: ✅
* `torch.nn.Softmax` (torch.nn.Module) available: ✅
* `torch.nn.Softmax2d` (torch.nn.Module) available: ❌
* `torch.nn.LogSoftmax` (torch.nn.Module) available: ✅
* `torch.nn.ELU` (torch.nn.Module) available: ❌
* `torch.nn.SELU` (torch.nn.Module) available: ❌
* `torch.nn.CELU` (torch.nn.Module) available: ❌
* `torch.nn.GLU` (torch.nn.Module) available: ❌
* `torch.nn.GELU` (torch.nn.Module) available: ✅
* `torch.nn.Hardshrink` (torch.nn.Module) available: ❌
* `torch.nn.LeakyReLU` (torch.nn.Module) available: ✅
* `torch.nn.LogSigmoid` (torch.nn.Module) available: ✅
* `torch.nn.Softplus` (torch.nn.Module) available: ❌
* `torch.nn.Softshrink` (torch.nn.Module) available: ❌
* `torch.nn.MultiheadAttention` (torch.nn.Module) available: ❌
* `torch.nn.PReLU` (torch.nn.Module) available: ❌
* `torch.nn.Softsign` (torch.nn.Module) available: ❌
* `torch.nn.Softmin` (torch.nn.Module) available: ❌
* `torch.nn.Tanhshrink` (torch.nn.Module) available: ❌
* `torch.nn.RReLU` (torch.nn.Module) available: ❌
* `torch.nn.L1Loss` (torch.nn.Module) available: ❌
* `torch.nn.NLLLoss` (torch.nn.Module) available: ❌
* `torch.nn.KLDivLoss` (torch.nn.Module) available: ❌
* `torch.nn.MSELoss` (torch.nn.Module) available: ❌
* `torch.nn.BCELoss` (torch.nn.Module) available: ❌
* `torch.nn.BCEWithLogitsLoss` (torch.nn.Module) available: ❌
* `torch.nn.NLLLoss2d` (torch.nn.Module) available: ❌
* `torch.nn.PoissonNLLLoss` (torch.nn.Module) available: ❌
* `torch.nn.CosineEmbeddingLoss` (torch.nn.Module) available: ❌
* `torch.nn.CTCLoss` (torch.nn.Module) available: ❌
* `torch.nn.HingeEmbeddingLoss` (torch.nn.Module) available: ❌
* `torch.nn.MarginRankingLoss` (torch.nn.Module) available: ❌
* `torch.nn.MultiLabelMarginLoss` (torch.nn.Module) available: ❌
* `torch.nn.MultiLabelSoftMarginLoss` (torch.nn.Module) available: ❌
* `torch.nn.MultiMarginLoss` (torch.nn.Module) available: ❌
* `torch.nn.SmoothL1Loss` (torch.nn.Module) available: ❌
* `torch.nn.SoftMarginLoss` (torch.nn.Module) available: ❌
* `torch.nn.CrossEntropyLoss` (torch.nn.Module) available: ❌
* `torch.nn.Container` (torch.nn.Module) available: ❌
* `torch.nn.Sequential` (torch.nn.Module) available: ✅
* `torch.nn.ModuleList` (torch.nn.Module) available: ✅
* `torch.nn.ModuleDict` (torch.nn.Module) available: ❌
* `torch.nn.ParameterList` (torch.nn.Module) available: ❌
* `torch.nn.ParameterDict` (torch.nn.Module) available: ❌
* `torch.nn.AvgPool1d` (torch.nn.Module) available: ❌
* `torch.nn.AvgPool2d` (torch.nn.Module) available: ❌
* `torch.nn.AvgPool3d` (torch.nn.Module) available: ❌
* `torch.nn.MaxPool1d` (torch.nn.Module) available: ✅
* `torch.nn.MaxPool2d` (torch.nn.Module) available: ✅
* `torch.nn.MaxPool3d` (torch.nn.Module) available: ❌
* `torch.nn.MaxUnpool1d` (torch.nn.Module) available: ❌
* `torch.nn.MaxUnpool2d` (torch.nn.Module) available: ❌
* `torch.nn.MaxUnpool3d` (torch.nn.Module) available: ❌
* `torch.nn.FractionalMaxPool2d` (torch.nn.Module) available: ❌
* `torch.nn.FractionalMaxPool3d` (torch.nn.Module) available: ❌
* `torch.nn.LPPool1d` (torch.nn.Module) available: ❌
* `torch.nn.LPPool2d` (torch.nn.Module) available: ❌
* `torch.nn.LocalResponseNorm` (torch.nn.Module) available: ❌
* `torch.nn.BatchNorm1d` (torch.nn.Module) available: ✅
* `torch.nn.BatchNorm2d` (torch.nn.Module) available: ❌
* `torch.nn.BatchNorm3d` (torch.nn.Module) available: ❌
* `torch.nn.InstanceNorm1d` (torch.nn.Module) available: ❌
* `torch.nn.InstanceNorm2d` (torch.nn.Module) available: ❌
* `torch.nn.InstanceNorm3d` (torch.nn.Module) available: ❌
* `torch.nn.LayerNorm` (torch.nn.Module) available: ✅
* `torch.nn.GroupNorm` (torch.nn.Module) available: ✅
* `torch.nn.SyncBatchNorm` (torch.nn.Module) available: ❌
* `torch.nn.Dropout` (torch.nn.Module) available: ✅
* `torch.nn.Dropout2d` (torch.nn.Module) available: ❌
* `torch.nn.Dropout3d` (torch.nn.Module) available: ❌
* `torch.nn.AlphaDropout` (torch.nn.Module) available: ❌
* `torch.nn.FeatureAlphaDropout` (torch.nn.Module) available: ❌
* `torch.nn.ReflectionPad1d` (torch.nn.Module) available: ✅
* `torch.nn.ReflectionPad2d` (torch.nn.Module) available: ❌
* `torch.nn.ReplicationPad2d` (torch.nn.Module) available: ❌
* `torch.nn.ReplicationPad1d` (torch.nn.Module) available: ✅
* `torch.nn.ReplicationPad3d` (torch.nn.Module) available: ❌
* `torch.nn.CrossMapLRN2d` (torch.nn.Module) available: ❌
* `torch.nn.Embedding` (torch.nn.Module) available: ✅
* `torch.nn.EmbeddingBag` (torch.nn.Module) available: ❌
* `torch.nn.RNNBase` (torch.nn.Module) available: ✅
* `torch.nn.RNN` (torch.nn.Module) available: ❌
* `torch.nn.LSTM` (torch.nn.Module) available: ✅
* `torch.nn.GRU` (torch.nn.Module) available: ❌
* `torch.nn.RNNCellBase` (torch.nn.Module) available: ❌
* `torch.nn.RNNCell` (torch.nn.Module) available: ❌
* `torch.nn.LSTMCell` (torch.nn.Module) available: ❌
* `torch.nn.GRUCell` (torch.nn.Module) available: ❌
* `torch.nn.PixelShuffle` (torch.nn.Module) available: ❌
* `torch.nn.Upsample` (torch.nn.Module) available: ❌
* `torch.nn.UpsamplingNearest2d` (torch.nn.Module) available: ❌
* `torch.nn.UpsamplingBilinear2d` (torch.nn.Module) available: ❌
* `torch.nn.PairwiseDistance` (torch.nn.Module) available: ❌
* `torch.nn.AdaptiveMaxPool1d` (torch.nn.Module) available: ❌
* `torch.nn.AdaptiveMaxPool2d` (torch.nn.Module) available: ❌
* `torch.nn.AdaptiveMaxPool3d` (torch.nn.Module) available: ❌
* `torch.nn.AdaptiveAvgPool1d` (torch.nn.Module) available: ❌
* `torch.nn.AdaptiveAvgPool2d` (torch.nn.Module) available: ❌
* `torch.nn.AdaptiveAvgPool3d` (torch.nn.Module) available: ❌
* `torch.nn.TripletMarginLoss` (torch.nn.Module) available: ❌
* `torch.nn.ZeroPad2d` (torch.nn.Module) available: ❌
* `torch.nn.ConstantPad1d` (torch.nn.Module) available: ✅
* `torch.nn.ConstantPad2d` (torch.nn.Module) available: ❌
* `torch.nn.ConstantPad3d` (torch.nn.Module) available: ❌
* `torch.nn.Bilinear` (torch.nn.Module) available: ❌
* `torch.nn.CosineSimilarity` (torch.nn.Module) available: ❌
* `torch.nn.Unfold` (torch.nn.Module) available: ❌
* `torch.nn.Fold` (torch.nn.Module) available: ❌
* `torch.nn.AdaptiveLogSoftmaxWithLoss` (torch.nn.Module) available: ❌
* `torch.nn.TransformerEncoder` (torch.nn.Module) available: ❌
* `torch.nn.TransformerDecoder` (torch.nn.Module) available: ❌
* `torch.nn.TransformerEncoderLayer` (torch.nn.Module) available: ❌
* `torch.nn.TransformerDecoderLayer` (torch.nn.Module) available: ❌
* `torch.nn.Transformer` (torch.nn.Module) available: ❌
* `torch.nn.Flatten` (torch.nn.Module) available: ✅
* `torch.nn.Unflatten` (torch.nn.Module) available: ✅
* `torch.nn.Hardsigmoid` (torch.nn.Module) available: ❌
* `torch.nn.Hardswish` (torch.nn.Module) available: ❌
* `torch.nn.SiLU` (torch.nn.Module) available: ❌
* `torch.nn.TripletMarginWithDistanceLoss` (torch.nn.Module) available: ❌
* `torch.nn.Parameter` (class) available: ✅
* `torch.nn.parallel` (module) available: ❌
* `torch.nn.DataParallel` (torch.nn.Module) available: ❌
* `torch.nn.intrinsic` (module) available: ❌
* `torch.nn.qat` (module) available: ❌
* `torch.nn.quantized` (module) available: ❌
* `torch.nn.functional.utils` (module) available: ❌
* `torch.nn.functional.grad` (module) available: ❌
* `torch.nn.functional.boolean_dispatch` (function) available: ❌
* `torch.nn.functional.has_torch_function` (function) available: ❌
* `torch.nn.functional.handle_torch_function` (function) available: ❌
* `torch.nn.functional.Tensor` (class) available: ✅
* `torch.nn.functional.conv1d` (function) available: ✅
* `torch.nn.functional.conv2d` (function) available: ✅
* `torch.nn.functional.conv3d` (function) available: ❌
* `torch.nn.functional.conv_transpose1d` (function) available: ✅
* `torch.nn.functional.conv_transpose2d` (function) available: ❌
* `torch.nn.functional.conv_transpose3d` (function) available: ❌
* `torch.nn.functional.conv_tbc` (function) available: ❌
* `torch.nn.functional.avg_pool1d` (function) available: ❌
* `torch.nn.functional.avg_pool2d` (function) available: ❌
* `torch.nn.functional.avg_pool3d` (function) available: ❌
* `torch.nn.functional.fractional_max_pool2d_with_indices` (function) available: ❌
* `torch.nn.functional.fractional_max_pool2d` (function) available: ❌
* `torch.nn.functional.fractional_max_pool3d_with_indices` (function) available: ❌
* `torch.nn.functional.fractional_max_pool3d` (function) available: ❌
* `torch.nn.functional.max_pool1d_with_indices` (function) available: ❌
* `torch.nn.functional.max_pool1d` (function) available: ❌
* `torch.nn.functional.max_pool2d_with_indices` (function) available: ❌
* `torch.nn.functional.max_pool2d` (function) available: ✅
* `torch.nn.functional.max_pool3d_with_indices` (function) available: ❌
* `torch.nn.functional.max_pool3d` (function) available: ❌
* `torch.nn.functional.max_unpool1d` (function) available: ❌
* `torch.nn.functional.max_unpool2d` (function) available: ❌
* `torch.nn.functional.max_unpool3d` (function) available: ❌
* `torch.nn.functional.lp_pool2d` (function) available: ❌
* `torch.nn.functional.lp_pool1d` (function) available: ❌
* `torch.nn.functional.adaptive_max_pool1d_with_indices` (function) available: ❌
* `torch.nn.functional.adaptive_max_pool1d` (function) available: ❌
* `torch.nn.functional.adaptive_max_pool2d_with_indices` (function) available: ❌
* `torch.nn.functional.adaptive_max_pool2d` (function) available: ❌
* `torch.nn.functional.adaptive_max_pool3d_with_indices` (function) available: ❌
* `torch.nn.functional.adaptive_max_pool3d` (function) available: ❌
* `torch.nn.functional.adaptive_avg_pool1d` (function) available: ❌
* `torch.nn.functional.adaptive_avg_pool2d` (function) available: ❌
* `torch.nn.functional.adaptive_avg_pool3d` (function) available: ❌
* `torch.nn.functional.dropout` (function) available: ✅
* `torch.nn.functional.alpha_dropout` (function) available: ❌
* `torch.nn.functional.dropout2d` (function) available: ❌
* `torch.nn.functional.dropout3d` (function) available: ❌
* `torch.nn.functional.feature_alpha_dropout` (function) available: ❌
* `torch.nn.functional.threshold` (function) available: ❌
* `torch.nn.functional.threshold_` (function) available: ❌
* `torch.nn.functional.relu` (function) available: ✅
* `torch.nn.functional.relu_` (function) available: ❌
* `torch.nn.functional.glu` (function) available: ❌
* `torch.nn.functional.hardtanh` (function) available: ❌
* `torch.nn.functional.hardtanh_` (function) available: ❌
* `torch.nn.functional.relu6` (function) available: ❌
* `torch.nn.functional.elu` (function) available: ❌
* `torch.nn.functional.elu_` (function) available: ❌
* `torch.nn.functional.selu` (function) available: ❌
* `torch.nn.functional.selu_` (function) available: ❌
* `torch.nn.functional.celu` (function) available: ❌
* `torch.nn.functional.celu_` (function) available: ❌
* `torch.nn.functional.leaky_relu` (function) available: ✅
* `torch.nn.functional.leaky_relu_` (function) available: ❌
* `torch.nn.functional.prelu` (function) available: ❌
* `torch.nn.functional.rrelu` (function) available: ❌
* `torch.nn.functional.rrelu_` (function) available: ❌
* `torch.nn.functional.logsigmoid` (function) available: ✅
* `torch.nn.functional.gelu` (function) available: ❌
* `torch.nn.functional.hardshrink` (function) available: ❌
* `torch.nn.functional.tanhshrink` (function) available: ❌
* `torch.nn.functional.softsign` (function) available: ❌
* `torch.nn.functional.softplus` (function) available: ❌
* `torch.nn.functional.softmin` (function) available: ❌
* `torch.nn.functional.softmax` (function) available: ✅
* `torch.nn.functional.gumbel_softmax` (function) available: ❌
* `torch.nn.functional.log_softmax` (function) available: ✅
* `torch.nn.functional.softshrink` (function) available: ❌
* `torch.nn.functional.tanh` (function) available: ✅
* `torch.nn.functional.sigmoid` (function) available: ✅
* `torch.nn.functional.hardsigmoid` (function) available: ❌
* `torch.nn.functional.linear` (function) available: ❌
* `torch.nn.functional.bilinear` (function) available: ❌
* `torch.nn.functional.silu` (function) available: ❌
* `torch.nn.functional.hardswish` (function) available: ❌
* `torch.nn.functional.embedding` (function) available: ❌
* `torch.nn.functional.embedding_bag` (function) available: ❌
* `torch.nn.functional.batch_norm` (function) available: ❌
* `torch.nn.functional.instance_norm` (function) available: ❌
* `torch.nn.functional.layer_norm` (function) available: ❌
* `torch.nn.functional.group_norm` (function) available: ✅
* `torch.nn.functional.local_response_norm` (function) available: ❌
* `torch.nn.functional.ctc_loss` (function) available: ❌
* `torch.nn.functional.nll_loss` (function) available: ❌
* `torch.nn.functional.poisson_nll_loss` (function) available: ❌
* `torch.nn.functional.kl_div` (function) available: ❌
* `torch.nn.functional.cross_entropy` (function) available: ❌
* `torch.nn.functional.binary_cross_entropy` (function) available: ❌
* `torch.nn.functional.binary_cross_entropy_with_logits` (function) available: ❌
* `torch.nn.functional.smooth_l1_loss` (function) available: ❌
* `torch.nn.functional.l1_loss` (function) available: ❌
* `torch.nn.functional.mse_loss` (function) available: ❌
* `torch.nn.functional.margin_ranking_loss` (function) available: ❌
* `torch.nn.functional.hinge_embedding_loss` (function) available: ❌
* `torch.nn.functional.multilabel_margin_loss` (function) available: ❌
* `torch.nn.functional.soft_margin_loss` (function) available: ❌
* `torch.nn.functional.multilabel_soft_margin_loss` (function) available: ❌
* `torch.nn.functional.cosine_embedding_loss` (function) available: ❌
* `torch.nn.functional.multi_margin_loss` (function) available: ❌
* `torch.nn.functional.pixel_shuffle` (function) available: ❌
* `torch.nn.functional.channel_shuffle` (function) available: ❌
* `torch.nn.functional.upsample` (function) available: ❌
* `torch.nn.functional.interpolate` (function) available: ❌
* `torch.nn.functional.upsample_nearest` (function) available: ❌
* `torch.nn.functional.upsample_bilinear` (function) available: ❌
* `torch.nn.functional.GRID_SAMPLE_INTERPOLATION_MODES` (dict) available: ❌
* `torch.nn.functional.GRID_SAMPLE_PADDING_MODES` (dict) available: ❌
* `torch.nn.functional.grid_sample` (function) available: ❌
* `torch.nn.functional.affine_grid` (function) available: ❌
* `torch.nn.functional.pad` (function) available: ✅
* `torch.nn.functional.pairwise_distance` (function) available: ❌
* `torch.nn.functional.pdist` (function) available: ❌
* `torch.nn.functional.cosine_similarity` (function) available: ❌
* `torch.nn.functional.one_hot` (function) available: ❌
* `torch.nn.functional.triplet_margin_loss` (function) available: ❌
* `torch.nn.functional.triplet_margin_with_distance_loss` (function) available: ❌
* `torch.nn.functional.normalize` (function) available: ✅
* `torch.nn.functional.assert_int_or_pair` (function) available: ❌
* `torch.nn.functional.unfold` (function) available: ❌
* `torch.nn.functional.fold` (function) available: ❌
* `torch.nn.functional.multi_head_attention_forward` (function) available: ❌
* `torch.platform` (module) available: ❌
* `torch.textwrap` (module) available: ❌
* `torch.ctypes` (module) available: ❌
* `torch.get_file_path` (function) available: ❌
* `torch.prepare_multiprocessing_environment` (function) available: ❌
* `torch.USE_RTLD_GLOBAL_WITH_LIBTORCH` (bool) available: ❌
* `torch.USE_GLOBAL_DEPS` (bool) available: ❌
* `torch.version` (module) available: ❌
* `torch.Set` (_GenericAlias) available: ❌
* `torch.Type` (class) available: ❌
* `torch.get_num_threads` (function) available: ❌
* `torch.set_num_threads` (function) available: ❌
* `torch.get_num_interop_threads` (function) available: ❌
* `torch.set_num_interop_threads` (function) available: ❌
* `torch.set_flush_denormal` (function) available: ❌
* `torch.get_default_dtype` (function) available: ❌
* `torch.is_grad_enabled` (function) available: ❌
* `torch.set_autocast_enabled` (function) available: ❌
* `torch.is_autocast_enabled` (function) available: ❌
* `torch.clear_autocast_cache` (function) available: ❌
* `torch.autocast_increment_nesting` (function) available: ❌
* `torch.autocast_decrement_nesting` (function) available: ❌
* `torch.set_anomaly_enabled` (function) available: ❌
* `torch.is_anomaly_enabled` (function) available: ❌
* `torch.Generator` (class) available: ❌
* `torch.FatalError` (class) available: ❌
* `torch.Size` (class) available: ✅
* `torch.dtype` (class) available: ✅
* `torch.finfo` (class) available: ❌
* `torch.iinfo` (class) available: ❌
* `torch.layout` (class) available: ❌
* `torch.memory_format` (class) available: ❌
* `torch.qscheme` (class) available: ❌
* `torch.device` (class) available: ✅
* `torch.JITException` (class) available: ❌
* `torch.IODescriptor` (class) available: ❌
* `torch.CompleteArgumentSpec` (class) available: ❌
* `torch.ArgumentSpec` (class) available: ❌
* `torch.Code` (class) available: ❌
* `torch.ExecutionPlan` (class) available: ❌
* `torch.Gradient` (class) available: ❌
* `torch.GraphExecutorState` (class) available: ❌
* `torch.PyTorchFileWriter` (class) available: ❌
* `torch.MobileOptimizerType` (class) available: ❌
* `torch.CONV_BN_FUSION` (MobileOptimizerType) available: ❌
* `torch.INSERT_FOLD_PREPACK_OPS` (MobileOptimizerType) available: ❌
* `torch.REMOVE_DROPOUT` (MobileOptimizerType) available: ❌
* `torch.FUSE_ADD_RELU` (MobileOptimizerType) available: ❌
* `torch.HOIST_CONV_PACKED_PARAMS` (MobileOptimizerType) available: ❌
* `torch.PyTorchFileReader` (class) available: ❌
* `torch.parse_ir` (function) available: ❌
* `torch.parse_schema` (function) available: ❌
* `torch.unify_type_list` (function) available: ❌
* `torch.FunctionSchema` (class) available: ❌
* `torch.Argument` (class) available: ❌
* `torch.Future` (class) available: ❌
* `torch.fork` (function) available: ❌
* `torch.wait` (function) available: ❌
* `torch.ScriptClass` (class) available: ❌
* `torch.Graph` (class) available: ❌
* `torch.Value` (class) available: ❌
* `torch.Block` (class) available: ❌
* `torch.Node` (class) available: ❌
* `torch.AnyType` (class) available: ❌
* `torch.NumberType` (class) available: ❌
* `torch.IntType` (class) available: ❌
* `torch.FloatType` (class) available: ❌
* `torch.TensorType` (class) available: ❌
* `torch.BoolType` (class) available: ❌
* `torch.StringType` (class) available: ❌
* `torch.DeviceObjType` (class) available: ❌
* `torch.PyObjectType` (class) available: ❌
* `torch.NoneType` (class) available: ❌
* `torch.TupleType` (class) available: ❌
* `torch.ListType` (class) available: ❌
* `torch.DictType` (class) available: ❌
* `torch.OptionalType` (class) available: ❌
* `torch.RRefType` (class) available: ❌
* `torch.FutureType` (class) available: ❌
* `torch.ClassType` (class) available: ❌
* `torch.EnumType` (class) available: ❌
* `torch.InterfaceType` (class) available: ❌
* `torch.Use` (class) available: ❌
* `torch.TracingState` (class) available: ❌
* `torch.Capsule` (class) available: ❌
* `torch.ScriptObject` (class) available: ❌
* `torch.DeepCopyMemoTable` (class) available: ❌
* `torch.ScriptModule` (class) available: ❌
* `torch.LiteScriptModule` (class) available: ❌
* `torch.ParameterDict` (class) available: ❌
* `torch.BufferDict` (class) available: ❌
* `torch.ModuleDict` (class) available: ❌
* `torch.ErrorReport` (class) available: ❌
* `torch.CompilationUnit` (class) available: ❌
* `torch.ScriptFunction` (class) available: ❌
* `torch.ScriptMethod` (class) available: ❌
* `torch.CallStack` (class) available: ❌
* `torch.parse_type_comment` (function) available: ❌
* `torch.merge_type_from_type_comment` (function) available: ❌
* `torch.import_ir_module` (function) available: ❌
* `torch.import_ir_module_from_buffer` (function) available: ❌
* `torch.FileCheck` (class) available: ❌
* `torch.ConcreteModuleTypeBuilder` (class) available: ❌
* `torch.ConcreteModuleType` (class) available: ❌
* `torch.LoggerBase` (class) available: ❌
* `torch.AggregationType` (class) available: ❌
* `torch.SUM` (AggregationType) available: ❌
* `torch.AVG` (AggregationType) available: ❌
* `torch.LockingLogger` (class) available: ❌
* `torch.NoopLogger` (class) available: ❌
* `torch.StaticRuntime` (class) available: ❌
* `torch.BenchmarkConfig` (class) available: ❌
* `torch.BenchmarkExecutionStats` (class) available: ❌
* `torch.ThroughputBenchmark` (class) available: ❌
* `torch.cpp` (module) available: ❌
* `torch.HalfStorageBase` (class) available: ❌
* `torch.QInt8StorageBase` (class) available: ❌
* `torch.QInt32StorageBase` (class) available: ❌
* `torch.CudaDoubleStorageBase` (class) available: ❌
* `torch.CudaFloatStorageBase` (class) available: ❌
* `torch.CudaHalfStorageBase` (class) available: ❌
* `torch.CudaLongStorageBase` (class) available: ❌
* `torch.CudaIntStorageBase` (class) available: ❌
* `torch.CudaShortStorageBase` (class) available: ❌
* `torch.CudaCharStorageBase` (class) available: ❌
* `torch.CudaByteStorageBase` (class) available: ❌
* `torch.CudaBoolStorageBase` (class) available: ❌
* `torch.CudaBFloat16StorageBase` (class) available: ❌
* `torch.CudaComplexDoubleStorageBase` (class) available: ❌
* `torch.CudaComplexFloatStorageBase` (class) available: ❌
* `torch.has_cudnn` (bool) available: ❌
* `torch.init_num_threads` (function) available: ❌
* `torch.has_openmp` (bool) available: ❌
* `torch.has_mkl` (bool) available: ❌
* `torch.has_lapack` (bool) available: ❌
* `torch.has_cuda` (bool) available: ❌
* `torch.has_mkldnn` (bool) available: ❌
* `torch.default_generator` (Generator) available: ❌
* `torch.DisableTorchFunction` (class) available: ❌
* `torch.typename` (function) available: ❌
* `torch.is_tensor` (function) available: ❌
* `torch.is_storage` (function) available: ❌
* `torch.set_default_tensor_type` (function) available: ❌
* `torch.set_default_dtype` (function) available: ❌
* `torch.set_deterministic` (function) available: ❌
* `torch.is_deterministic` (function) available: ❌
* `torch.utils` (module) available: ✅
* `torch.tensor` (function) available: ✅
* `torch.Tensor` (class) available: ✅
* `torch.storage` (module) available: ❌
* `torch.DoubleStorage` (class) available: ❌
* `torch.FloatStorage` (class) available: ❌
* `torch.HalfStorage` (class) available: ❌
* `torch.LongStorage` (class) available: ❌
* `torch.IntStorage` (class) available: ❌
* `torch.ShortStorage` (class) available: ❌
* `torch.CharStorage` (class) available: ❌
* `torch.ByteStorage` (class) available: ❌
* `torch.BoolStorage` (class) available: ❌
* `torch.BFloat16Storage` (class) available: ❌
* `torch.ComplexDoubleStorage` (class) available: ❌
* `torch.ComplexFloatStorage` (class) available: ❌
* `torch.QUInt8Storage` (class) available: ❌
* `torch.QInt8Storage` (class) available: ❌
* `torch.QInt32Storage` (class) available: ❌
* `torch.random` (module) available: ❌
* `torch.set_rng_state` (function) available: ❌
* `torch.get_rng_state` (function) available: ❌
* `torch.manual_seed` (function) available: ❌
* `torch.initial_seed` (function) available: ❌
* `torch.seed` (function) available: ❌
* `torch.types` (module) available: ❌
* `torch.serialization` (module) available: ✅
* `torch.save` (function) available: ❌
* `torch.load` (function) available: ✅
* `torch.set_printoptions` (function) available: ❌
* `torch.strided` (layout) available: ❌
* `torch.sparse_coo` (layout) available: ❌
* `torch.preserve_format` (memory_format) available: ❌
* `torch.contiguous_format` (memory_format) available: ❌
* `torch.channels_last` (memory_format) available: ❌
* `torch.channels_last_3d` (memory_format) available: ❌
* `torch.per_tensor_affine` (qscheme) available: ❌
* `torch.per_channel_affine` (qscheme) available: ❌
* `torch.per_tensor_symmetric` (qscheme) available: ❌
* `torch.per_channel_symmetric` (qscheme) available: ❌
* `torch.per_channel_affine_float_qparams` (qscheme) available: ❌
* `torch.uint8` (dtype) available: ❌
* `torch.int8` (dtype) available: ❌
* `torch.int16` (dtype) available: ❌
* `torch.short` (dtype) available: ❌
* `torch.int32` (dtype) available: ❌
* `torch.int` (dtype) available: ❌
* `torch.int64` (dtype) available: ❌
* `torch.long` (dtype) available: ❌
* `torch.float16` (dtype) available: ❌
* `torch.half` (dtype) available: ❌
* `torch.float32` (dtype) available: ❌
* `torch.float` (dtype) available: ❌
* `torch.float64` (dtype) available: ❌
* `torch.double` (dtype) available: ❌
* `torch.complex32` (dtype) available: ❌
* `torch.complex64` (dtype) available: ❌
* `torch.cfloat` (dtype) available: ❌
* `torch.complex128` (dtype) available: ❌
* `torch.cdouble` (dtype) available: ❌
* `torch.bool` (dtype) available: ❌
* `torch.qint8` (dtype) available: ❌
* `torch.quint8` (dtype) available: ❌
* `torch.qint32` (dtype) available: ❌
* `torch.bfloat16` (dtype) available: ❌
* `torch.Storage` (class) available: ❌
* `torch.ByteTensor` (class) available: ❌
* `torch.CharTensor` (class) available: ❌
* `torch.DoubleTensor` (class) available: ❌
* `torch.FloatTensor` (class) available: ✅
* `torch.IntTensor` (class) available: ❌
* `torch.LongTensor` (class) available: ✅
* `torch.ShortTensor` (class) available: ❌
* `torch.HalfTensor` (class) available: ❌
* `torch.BoolTensor` (class) available: ❌
* `torch.BFloat16Tensor` (class) available: ❌
* `torch.cuda` (module) available: ✅
* `torch.sparse` (module) available: ❌
* `torch.name` (str) available: ❌
* `torch.abs` (function) available: ✅
* `torch.abs_` (function) available: ❌
* `torch.absolute` (function) available: ❌
* `torch.acos` (function) available: ❌
* `torch.acos_` (function) available: ❌
* `torch.acosh` (function) available: ❌
* `torch.acosh_` (function) available: ❌
* `torch.adaptive_avg_pool1d` (function) available: ❌
* `torch.adaptive_max_pool1d` (function) available: ❌
* `torch.add` (function) available: ✅
* `torch.addbmm` (function) available: ❌
* `torch.addcdiv` (function) available: ❌
* `torch.addcmul` (function) available: ❌
* `torch.addmm` (function) available: ❌
* `torch.addmv` (function) available: ❌
* `torch.addmv_` (function) available: ❌
* `torch.addr` (function) available: ❌
* `torch.affine_grid_generator` (function) available: ❌
* `torch.align_tensors` (function) available: ❌
* `torch.all` (function) available: ❌
* `torch.allclose` (function) available: ❌
* `torch.alpha_dropout` (function) available: ❌
* `torch.alpha_dropout_` (function) available: ❌
* `torch.amax` (function) available: ❌
* `torch.amin` (function) available: ❌
* `torch.angle` (function) available: ❌
* `torch.any` (function) available: ❌
* `torch.arange` (function) available: ❌
* `torch.arccos` (function) available: ❌
* `torch.arccos_` (function) available: ❌
* `torch.arccosh` (function) available: ❌
* `torch.arccosh_` (function) available: ❌
* `torch.arcsin` (function) available: ❌
* `torch.arcsin_` (function) available: ❌
* `torch.arcsinh` (function) available: ❌
* `torch.arcsinh_` (function) available: ❌
* `torch.arctan` (function) available: ❌
* `torch.arctan_` (function) available: ❌
* `torch.arctanh` (function) available: ❌
* `torch.arctanh_` (function) available: ❌
* `torch.argmax` (function) available: ❌
* `torch.argmin` (function) available: ❌
* `torch.argsort` (function) available: ❌
* `torch.as_strided` (function) available: ❌
* `torch.as_strided_` (function) available: ❌
* `torch.as_tensor` (function) available: ✅
* `torch.asin` (function) available: ❌
* `torch.asin_` (function) available: ❌
* `torch.asinh` (function) available: ❌
* `torch.asinh_` (function) available: ❌
* `torch.atan` (function) available: ❌
* `torch.atan2` (function) available: ❌
* `torch.atan_` (function) available: ❌
* `torch.atanh` (function) available: ❌
* `torch.atanh_` (function) available: ❌
* `torch.atleast_1d` (function) available: ❌
* `torch.atleast_2d` (function) available: ❌
* `torch.atleast_3d` (function) available: ❌
* `torch.avg_pool1d` (function) available: ❌
* `torch.baddbmm` (function) available: ❌
* `torch.bartlett_window` (function) available: ❌
* `torch.batch_norm` (function) available: ❌
* `torch.batch_norm_backward_elemt` (function) available: ❌
* `torch.batch_norm_backward_reduce` (function) available: ❌
* `torch.batch_norm_elemt` (function) available: ❌
* `torch.batch_norm_gather_stats` (function) available: ❌
* `torch.batch_norm_gather_stats_with_counts` (function) available: ❌
* `torch.batch_norm_stats` (function) available: ❌
* `torch.batch_norm_update_stats` (function) available: ❌
* `torch.bernoulli` (function) available: ❌
* `torch.bilinear` (function) available: ❌
* `torch.binary_cross_entropy_with_logits` (function) available: ❌
* `torch.bincount` (function) available: ❌
* `torch.binomial` (function) available: ❌
* `torch.bitwise_and` (function) available: ❌
* `torch.bitwise_not` (function) available: ❌
* `torch.bitwise_or` (function) available: ❌
* `torch.bitwise_xor` (function) available: ❌
* `torch.blackman_window` (function) available: ❌
* `torch.block_diag` (function) available: ❌
* `torch.bmm` (function) available: ❌
* `torch.broadcast_tensors` (function) available: ❌
* `torch.bucketize` (function) available: ❌
* `torch.can_cast` (function) available: ❌
* `torch.cartesian_prod` (function) available: ❌
* `torch.cat` (function) available: ❌
* `torch.cdist` (function) available: ❌
* `torch.ceil` (function) available: ❌
* `torch.ceil_` (function) available: ❌
* `torch.celu` (function) available: ❌
* `torch.celu_` (function) available: ❌
* `torch.chain_matmul` (function) available: ❌
* `torch.channel_shuffle` (function) available: ❌
* `torch.cholesky` (function) available: ❌
* `torch.cholesky_inverse` (function) available: ❌
* `torch.cholesky_solve` (function) available: ❌
* `torch.choose_qparams_optimized` (function) available: ❌
* `torch.chunk` (function) available: ✅
* `torch.clamp` (function) available: ❌
* `torch.clamp_` (function) available: ❌
* `torch.clamp_max` (function) available: ❌
* `torch.clamp_max_` (function) available: ❌
* `torch.clamp_min` (function) available: ❌
* `torch.clamp_min_` (function) available: ❌
* `torch.clip` (function) available: ❌
* `torch.clip_` (function) available: ❌
* `torch.clone` (function) available: ❌
* `torch.combinations` (function) available: ❌
* `torch.complex` (function) available: ❌
* `torch.conj` (function) available: ❌
* `torch.constant_pad_nd` (function) available: ❌
* `torch.conv1d` (function) available: ✅
* `torch.conv2d` (function) available: ✅
* `torch.conv3d` (function) available: ❌
* `torch.conv_tbc` (function) available: ❌
* `torch.conv_transpose1d` (function) available: ✅
* `torch.conv_transpose2d` (function) available: ❌
* `torch.conv_transpose3d` (function) available: ❌
* `torch.convolution` (function) available: ❌
* `torch.cos` (function) available: ❌
* `torch.cos_` (function) available: ❌
* `torch.cosh` (function) available: ❌
* `torch.cosh_` (function) available: ❌
* `torch.cosine_embedding_loss` (function) available: ❌
* `torch.cosine_similarity` (function) available: ❌
* `torch.count_nonzero` (function) available: ❌
* `torch.cross` (function) available: ❌
* `torch.ctc_loss` (function) available: ❌
* `torch.cudnn_affine_grid_generator` (function) available: ❌
* `torch.cudnn_batch_norm` (function) available: ❌
* `torch.cudnn_convolution` (function) available: ❌
* `torch.cudnn_convolution_transpose` (function) available: ❌
* `torch.cudnn_grid_sampler` (function) available: ❌
* `torch.cudnn_is_acceptable` (function) available: ❌
* `torch.cummax` (function) available: ❌
* `torch.cummin` (function) available: ❌
* `torch.cumprod` (function) available: ❌
* `torch.cumsum` (function) available: ❌
* `torch.deg2rad` (function) available: ❌
* `torch.deg2rad_` (function) available: ❌
* `torch.dequantize` (function) available: ❌
* `torch.det` (function) available: ❌
* `torch.detach` (function) available: ❌
* `torch.detach_` (function) available: ❌
* `torch.diag` (function) available: ❌
* `torch.diag_embed` (function) available: ❌
* `torch.diagflat` (function) available: ❌
* `torch.diagonal` (function) available: ❌
* `torch.digamma` (function) available: ❌
* `torch.dist` (function) available: ❌
* `torch.div` (function) available: ❌
* `torch.divide` (function) available: ❌
* `torch.dot` (function) available: ❌
* `torch.dropout` (function) available: ✅
* `torch.dropout_` (function) available: ❌
* `torch.dsmm` (function) available: ❌
* `torch.dstack` (function) available: ❌
* `torch.eig` (function) available: ❌
* `torch.einsum` (function) available: ❌
* `torch.embedding` (function) available: ❌
* `torch.embedding_bag` (function) available: ❌
* `torch.embedding_renorm_` (function) available: ❌
* `torch.empty` (function) available: ❌
* `torch.empty_like` (function) available: ❌
* `torch.empty_meta` (function) available: ❌
* `torch.empty_quantized` (function) available: ❌
* `torch.empty_strided` (function) available: ❌
* `torch.eq` (function) available: ❌
* `torch.equal` (function) available: ❌
* `torch.erf` (function) available: ❌
* `torch.erf_` (function) available: ❌
* `torch.erfc` (function) available: ❌
* `torch.erfc_` (function) available: ❌
* `torch.erfinv` (function) available: ❌
* `torch.exp` (function) available: ❌
* `torch.exp2` (function) available: ❌
* `torch.exp2_` (function) available: ❌
* `torch.exp_` (function) available: ❌
* `torch.expm1` (function) available: ❌
* `torch.expm1_` (function) available: ❌
* `torch.eye` (function) available: ❌
* `torch.fake_quantize_per_channel_affine` (function) available: ❌
* `torch.fake_quantize_per_tensor_affine` (function) available: ❌
* `torch.fbgemm_linear_fp16_weight` (function) available: ❌
* `torch.fbgemm_linear_fp16_weight_fp32_activation` (function) available: ❌
* `torch.fbgemm_linear_int8_weight` (function) available: ❌
* `torch.fbgemm_linear_int8_weight_fp32_activation` (function) available: ❌
* `torch.fbgemm_linear_quantize_weight` (function) available: ❌
* `torch.fbgemm_pack_gemm_matrix_fp16` (function) available: ❌
* `torch.fbgemm_pack_quantized_matrix` (function) available: ❌
* `torch.feature_alpha_dropout` (function) available: ❌
* `torch.feature_alpha_dropout_` (function) available: ❌
* `torch.feature_dropout` (function) available: ❌
* `torch.feature_dropout_` (function) available: ❌
* `torch.fft` (function) available: ❌
* `torch.fill_` (function) available: ❌
* `torch.fix` (function) available: ❌
* `torch.fix_` (function) available: ❌
* `torch.flatten` (function) available: ✅
* `torch.flip` (function) available: ❌
* `torch.fliplr` (function) available: ❌
* `torch.flipud` (function) available: ❌
* `torch.floor` (function) available: ❌
* `torch.floor_` (function) available: ❌
* `torch.floor_divide` (function) available: ❌
* `torch.fmod` (function) available: ❌
* `torch.frac` (function) available: ❌
* `torch.frac_` (function) available: ❌
* `torch.frobenius_norm` (function) available: ❌
* `torch.from_file` (function) available: ❌
* `torch.from_numpy` (function) available: ✅
* `torch.full` (function) available: ❌
* `torch.full_like` (function) available: ❌
* `torch.gather` (function) available: ❌
* `torch.gcd` (function) available: ❌
* `torch.gcd_` (function) available: ❌
* `torch.ge` (function) available: ❌
* `torch.geqrf` (function) available: ❌
* `torch.ger` (function) available: ❌
* `torch.get_device` (function) available: ❌
* `torch.greater` (function) available: ❌
* `torch.greater_equal` (function) available: ✅
* `torch.grid_sampler` (function) available: ❌
* `torch.grid_sampler_2d` (function) available: ❌
* `torch.grid_sampler_3d` (function) available: ❌
* `torch.group_norm` (function) available: ✅
* `torch.gru` (function) available: ❌
* `torch.gru_cell` (function) available: ❌
* `torch.gt` (function) available: ❌
* `torch.hamming_window` (function) available: ❌
* `torch.hann_window` (function) available: ❌
* `torch.hardshrink` (function) available: ❌
* `torch.heaviside` (function) available: ❌
* `torch.hinge_embedding_loss` (function) available: ❌
* `torch.histc` (function) available: ❌
* `torch.hsmm` (function) available: ❌
* `torch.hspmm` (function) available: ❌
* `torch.hstack` (function) available: ❌
* `torch.hypot` (function) available: ❌
* `torch.i0` (function) available: ❌
* `torch.i0_` (function) available: ❌
* `torch.ifft` (function) available: ❌
* `torch.imag` (function) available: ❌
* `torch.index_add` (function) available: ❌
* `torch.index_copy` (function) available: ❌
* `torch.index_fill` (function) available: ❌
* `torch.index_put` (function) available: ❌
* `torch.index_put_` (function) available: ❌
* `torch.index_select` (function) available: ❌
* `torch.instance_norm` (function) available: ❌
* `torch.int_repr` (function) available: ❌
* `torch.inverse` (function) available: ❌
* `torch.irfft` (function) available: ❌
* `torch.is_complex` (function) available: ❌
* `torch.is_distributed` (function) available: ❌
* `torch.is_floating_point` (function) available: ❌
* `torch.is_nonzero` (function) available: ❌
* `torch.is_same_size` (function) available: ❌
* `torch.is_signed` (function) available: ❌
* `torch.is_vulkan_available` (function) available: ❌
* `torch.isclose` (function) available: ❌
* `torch.isfinite` (function) available: ❌
* `torch.isinf` (function) available: ❌
* `torch.isnan` (function) available: ❌
* `torch.isneginf` (function) available: ❌
* `torch.isposinf` (function) available: ❌
* `torch.isreal` (function) available: ❌
* `torch.istft` (function) available: ❌
* `torch.kaiser_window` (function) available: ❌
* `torch.kl_div` (function) available: ❌
* `torch.kthvalue` (function) available: ❌
* `torch.layer_norm` (function) available: ❌
* `torch.lcm` (function) available: ❌
* `torch.lcm_` (function) available: ❌
* `torch.le` (function) available: ❌
* `torch.lerp` (function) available: ❌
* `torch.less` (function) available: ❌
* `torch.less_equal` (function) available: ❌
* `torch.lgamma` (function) available: ❌
* `torch.linspace` (function) available: ❌
* `torch.log` (function) available: ✅
* `torch.log10` (function) available: ❌
* `torch.log10_` (function) available: ❌
* `torch.log1p` (function) available: ❌
* `torch.log1p_` (function) available: ❌
* `torch.log2` (function) available: ❌
* `torch.log2_` (function) available: ❌
* `torch.log_` (function) available: ❌
* `torch.log_softmax` (function) available: ✅
* `torch.logaddexp` (function) available: ❌
* `torch.logaddexp2` (function) available: ❌
* `torch.logcumsumexp` (function) available: ❌
* `torch.logdet` (function) available: ❌
* `torch.logical_and` (function) available: ❌
* `torch.logical_not` (function) available: ❌
* `torch.logical_or` (function) available: ❌
* `torch.logical_xor` (function) available: ❌
* `torch.logit` (function) available: ❌
* `torch.logit_` (function) available: ❌
* `torch.logspace` (function) available: ❌
* `torch.logsumexp` (function) available: ❌
* `torch.lstm` (function) available: ❌
* `torch.lstm_cell` (function) available: ❌
* `torch.lstsq` (function) available: ❌
* `torch.lt` (function) available: ❌
* `torch.lu_solve` (function) available: ❌
* `torch.margin_ranking_loss` (function) available: ❌
* `torch.masked_fill` (function) available: ❌
* `torch.masked_scatter` (function) available: ❌
* `torch.masked_select` (function) available: ❌
* `torch.matmul` (function) available: ❌
* `torch.matrix_exp` (function) available: ❌
* `torch.matrix_power` (function) available: ❌
* `torch.matrix_rank` (function) available: ❌
* `torch.max` (function) available: ✅
* `torch.max_pool1d` (function) available: ❌
* `torch.max_pool1d_with_indices` (function) available: ❌
* `torch.max_pool2d` (function) available: ✅
* `torch.max_pool3d` (function) available: ❌
* `torch.maximum` (function) available: ❌
* `torch.mean` (function) available: ❌
* `torch.median` (function) available: ❌
* `torch.meshgrid` (function) available: ❌
* `torch.min` (function) available: ❌
* `torch.minimum` (function) available: ❌
* `torch.miopen_batch_norm` (function) available: ❌
* `torch.miopen_convolution` (function) available: ❌
* `torch.miopen_convolution_transpose` (function) available: ❌
* `torch.miopen_depthwise_convolution` (function) available: ❌
* `torch.miopen_rnn` (function) available: ❌
* `torch.mkldnn_adaptive_avg_pool2d` (function) available: ❌
* `torch.mkldnn_convolution` (function) available: ❌
* `torch.mkldnn_convolution_backward_weights` (function) available: ❌
* `torch.mkldnn_max_pool2d` (function) available: ❌
* `torch.mkldnn_max_pool3d` (function) available: ❌
* `torch.mm` (function) available: ❌
* `torch.mode` (function) available: ❌
* `torch.movedim` (function) available: ✅
* `torch.mul` (function) available: ✅
* `torch.multinomial` (function) available: ❌
* `torch.multiply` (function) available: ❌
* `torch.mv` (function) available: ❌
* `torch.mvlgamma` (function) available: ❌
* `torch.nanquantile` (function) available: ❌
* `torch.nansum` (function) available: ❌
* `torch.narrow` (function) available: ❌
* `torch.native_batch_norm` (function) available: ❌
* `torch.native_group_norm` (function) available: ❌
* `torch.native_layer_norm` (function) available: ❌
* `torch.native_norm` (function) available: ❌
* `torch.ne` (function) available: ❌
* `torch.neg` (function) available: ❌
* `torch.neg_` (function) available: ❌
* `torch.negative` (function) available: ❌
* `torch.negative_` (function) available: ❌
* `torch.nextafter` (function) available: ❌
* `torch.nonzero` (function) available: ❌
* `torch.norm` (function) available: ✅
* `torch.norm_except_dim` (function) available: ✅
* `torch.normal` (function) available: ❌
* `torch.not_equal` (function) available: ❌
* `torch.nuclear_norm` (function) available: ❌
* `torch.numel` (function) available: ❌
* `torch.ones` (function) available: ✅
* `torch.ones_like` (function) available: ❌
* `torch.orgqr` (function) available: ❌
* `torch.ormqr` (function) available: ❌
* `torch.outer` (function) available: ❌
* `torch.pairwise_distance` (function) available: ❌
* `torch.pdist` (function) available: ❌
* `torch.pinverse` (function) available: ❌
* `torch.pixel_shuffle` (function) available: ❌
* `torch.poisson` (function) available: ❌
* `torch.poisson_nll_loss` (function) available: ❌
* `torch.polar` (function) available: ❌
* `torch.polygamma` (function) available: ❌
* `torch.pow` (function) available: ✅
* `torch.prelu` (function) available: ❌
* `torch.prod` (function) available: ❌
* `torch.promote_types` (function) available: ✅
* `torch.q_per_channel_axis` (function) available: ❌
* `torch.q_per_channel_scales` (function) available: ❌
* `torch.q_per_channel_zero_points` (function) available: ❌
* `torch.q_scale` (function) available: ❌
* `torch.q_zero_point` (function) available: ❌
* `torch.qr` (function) available: ❌
* `torch.quantile` (function) available: ❌
* `torch.quantize_per_channel` (function) available: ❌
* `torch.quantize_per_tensor` (function) available: ❌
* `torch.quantized_batch_norm` (function) available: ❌
* `torch.quantized_gru_cell` (function) available: ❌
* `torch.quantized_lstm_cell` (function) available: ❌
* `torch.quantized_max_pool1d` (function) available: ❌
* `torch.quantized_max_pool2d` (function) available: ❌
* `torch.quantized_rnn_relu_cell` (function) available: ❌
* `torch.quantized_rnn_tanh_cell` (function) available: ❌
* `torch.rad2deg` (function) available: ❌
* `torch.rad2deg_` (function) available: ❌
* `torch.rand` (function) available: ❌
* `torch.rand_like` (function) available: ❌
* `torch.randint` (function) available: ❌
* `torch.randint_like` (function) available: ❌
* `torch.randn` (function) available: ❌
* `torch.randn_like` (function) available: ❌
* `torch.randperm` (function) available: ❌
* `torch.range` (function) available: ❌
* `torch.real` (function) available: ❌
* `torch.reciprocal` (function) available: ❌
* `torch.reciprocal_` (function) available: ❌
* `torch.relu` (function) available: ✅
* `torch.relu_` (function) available: ❌
* `torch.remainder` (function) available: ❌
* `torch.renorm` (function) available: ❌
* `torch.repeat_interleave` (function) available: ❌
* `torch.reshape` (function) available: ✅
* `torch.resize_as_` (function) available: ❌
* `torch.result_type` (function) available: ✅
* `torch.rfft` (function) available: ❌
* `torch.rnn_relu` (function) available: ❌
* `torch.rnn_relu_cell` (function) available: ❌
* `torch.rnn_tanh` (function) available: ❌
* `torch.rnn_tanh_cell` (function) available: ❌
* `torch.roll` (function) available: ❌
* `torch.rot90` (function) available: ❌
* `torch.round` (function) available: ❌
* `torch.round_` (function) available: ❌
* `torch.rrelu` (function) available: ❌
* `torch.rrelu_` (function) available: ❌
* `torch.rsqrt` (function) available: ❌
* `torch.rsqrt_` (function) available: ❌
* `torch.rsub` (function) available: ❌
* `torch.saddmm` (function) available: ❌
* `torch.scalar_tensor` (function) available: ❌
* `torch.scatter` (function) available: ❌
* `torch.scatter_add` (function) available: ❌
* `torch.searchsorted` (function) available: ❌
* `torch.select` (function) available: ❌
* `torch.selu` (function) available: ❌
* `torch.selu_` (function) available: ❌
* `torch.sgn` (function) available: ❌
* `torch.sigmoid` (function) available: ✅
* `torch.sigmoid_` (function) available: ❌
* `torch.sign` (function) available: ❌
* `torch.signbit` (function) available: ❌
* `torch.sin` (function) available: ❌
* `torch.sin_` (function) available: ❌
* `torch.sinh` (function) available: ❌
* `torch.sinh_` (function) available: ❌
* `torch.slogdet` (function) available: ❌
* `torch.smm` (function) available: ❌
* `torch.softmax` (function) available: ✅
* `torch.solve` (function) available: ❌
* `torch.sort` (function) available: ❌
* `torch.sparse_coo_tensor` (function) available: ❌
* `torch.split` (function) available: ❌
* `torch.split_with_sizes` (function) available: ❌
* `torch.spmm` (function) available: ❌
* `torch.sqrt` (function) available: ❌
* `torch.sqrt_` (function) available: ❌
* `torch.square` (function) available: ❌
* `torch.square_` (function) available: ❌
* `torch.squeeze` (function) available: ❌
* `torch.sspaddmm` (function) available: ❌
* `torch.stack` (function) available: ❌
* `torch.std` (function) available: ❌
* `torch.std_mean` (function) available: ❌
* `torch.stft` (function) available: ❌
* `torch.sub` (function) available: ✅
* `torch.subtract` (function) available: ❌
* `torch.sum` (function) available: ❌
* `torch.svd` (function) available: ❌
* `torch.symeig` (function) available: ❌
* `torch.t` (function) available: ❌
* `torch.take` (function) available: ❌
* `torch.tan` (function) available: ❌
* `torch.tan_` (function) available: ❌
* `torch.tanh` (function) available: ✅
* `torch.tanh_` (function) available: ❌
* `torch.tensordot` (function) available: ❌
* `torch.threshold` (function) available: ❌
* `torch.threshold_` (function) available: ❌
* `torch.topk` (function) available: ❌
* `torch.trace` (function) available: ❌
* `torch.transpose` (function) available: ✅
* `torch.trapz` (function) available: ❌
* `torch.triangular_solve` (function) available: ❌
* `torch.tril` (function) available: ❌
* `torch.tril_indices` (function) available: ❌
* `torch.triplet_margin_loss` (function) available: ❌
* `torch.triu` (function) available: ❌
* `torch.triu_indices` (function) available: ❌
* `torch.true_divide` (function) available: ❌
* `torch.trunc` (function) available: ❌
* `torch.trunc_` (function) available: ❌
* `torch.unbind` (function) available: ❌
* `torch.unique_consecutive` (function) available: ❌
* `torch.unsafe_chunk` (function) available: ❌
* `torch.unsafe_split` (function) available: ❌
* `torch.unsafe_split_with_sizes` (function) available: ❌
* `torch.unsqueeze` (function) available: ❌
* `torch.vander` (function) available: ❌
* `torch.var` (function) available: ❌
* `torch.var_mean` (function) available: ❌
* `torch.vdot` (function) available: ❌
* `torch.view_as_complex` (function) available: ❌
* `torch.view_as_real` (function) available: ❌
* `torch.vstack` (function) available: ❌
* `torch.where` (function) available: ❌
* `torch.zero_` (function) available: ❌
* `torch.zeros` (function) available: ✅
* `torch.zeros_like` (function) available: ❌
* `torch.distributed` (module) available: ❌
* `torch.testing` (module) available: ❌
* `torch.overrides` (module) available: ❌
* `torch.futures` (module) available: ❌
* `torch.autograd` (module) available: ✅
* `torch.nn` (module) available: ✅
* `torch.functional` (module) available: ❌
* `torch.lu` (function) available: ❌
* `torch.lu_unpack` (function) available: ❌
* `torch.pca_lowrank` (function) available: ❌
* `torch.svd_lowrank` (function) available: ❌
* `torch.unique` (function) available: ❌
* `torch.no_grad` (class) available: ✅
* `torch.enable_grad` (class) available: ❌
* `torch.set_grad_enabled` (class) available: ❌
* `torch.backends` (module) available: ❌
* `torch.jit` (module) available: ✅
* `torch.optim` (module) available: ❌
* `torch.multiprocessing` (module) available: ❌
* `torch.onnx` (module) available: ✅
* `torch.linalg` (module) available: ❌
* `torch.hub` (module) available: ❌
* `torch.distributions` (module) available: ❌
* `torch.quantization` (module) available: ❌
* `torch.compiled_with_cxx11_abi` (function) available: ❌
* `torch.ops` (_Ops) available: ❌
* `torch.classes` (_Classes) available: ❌
* `torch.quasirandom` (module) available: ❌
* `torch.legacy_contiguous_format` (memory_format) available: ❌
* `torch.lobpcg` (function) available: ❌
* `torch.quantized_lstm` (function) available: ❌
* `torch.quantized_gru` (function) available: ❌
* `torch.Assert` (function) available: ❌
