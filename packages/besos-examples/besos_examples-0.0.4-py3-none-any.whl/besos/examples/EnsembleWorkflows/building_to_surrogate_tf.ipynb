{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Work Flow Advanced Interface\n",
    "\n",
    "\n",
    "In this notebook we will go over some of the basic work flow to create a a surrogate model from an EnergyPlus simulation. We will train a neural network to find daily electricity output based on window to wall ratio and solar gain coefficient. Finally we will use this surrogate model to do an optimization of the building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](image/flow_diagram.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#!pip install besos --user\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import tensorflow as tf\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "from besos import eppy_funcs as ef, sampling\n",
    "from besos.evaluator import EvaluatorEP, EvaluatorGeneric\n",
    "from besos.parameters import FieldSelector, Parameter, RangeParameter, wwr\n",
    "from besos.problem import EPProblem\n",
    "from dask.distributed import Client\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly import express as px\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn for pairplot\n",
    "#!pip install --upgrade tensorflow --user\n",
    "\n",
    "# Use some functions from tensorflow_docs\n",
    "#!pip install git+https://github.com/tensorflow/docs --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Set up the building from idf\n",
    "\n",
    "The building is defined by the Information Data File (IDF) or using the new EnergyPlus format (epJSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the IDF file\n",
    "building = ef.get_building(\"Medium_Office.idf\")\n",
    "building.view_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can convert an idf to epJSON using the following code.\n",
    "# !energyplus -c \"Medium_Office.idf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "# (2) Evaluator\n",
    "## Set up the inputs and outputs of your exploration\n",
    "\n",
    "Defines how we will evaluate the building;\n",
    "- what external weather conditions is the building experiencing,\n",
    "- what properties of the building will we be changing, and\n",
    "- what are some of the performance metrics of the building that we want to explore.\n",
    "\n",
    "The weather conditions are specified in the EnergyPlus Weather File (EWP) file. The properties we will change in the building will be defined in the parameter space. In the objectives we will specify the what output performance metrics we wish to extract such that we can explore them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building.idfobjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for materials in building.idfobjects[\"MATERIAL:NOMASS\"]:\n",
    "#     print(\"{} {}\".format(materials.Name,materials.Thermal_Resistance))\n",
    "\n",
    "# for materials in building.idfobjects[\"BUILDINGSURFACE:DETAILED\"]:\n",
    "#     if materials.Sun_Exposure!=\"NoSun\": print(materials.Construction_Name )\n",
    "\n",
    "# for materials in building.idfobjects['CONSTRUCTION']:\n",
    "#     if materials.Name==\"BTAP-Ext-Wall-Mass:U-0.315\": print(materials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](image/setting_up_the_evaluator.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we change all the external insulation of the building\n",
    "insu1 = FieldSelector(\n",
    "    class_name=\"MATERIAL:NOMASS\",\n",
    "    object_name=\"Typical Insulation 2\",\n",
    "    field_name=\"Thermal Resistance\",\n",
    ")\n",
    "\n",
    "\n",
    "# Setup the parameters, Solar Heat Gain Coefficient\n",
    "parameters = [\n",
    "    Parameter(\n",
    "        FieldSelector(\"Window\", \"*\", \"Solar Heat Gain Coefficient\"),\n",
    "        value_descriptor=RangeParameter(0.01, 0.99),\n",
    "        name=\"Solar Gain Coefficient\",\n",
    "    ),\n",
    "    Parameter(\n",
    "        insu1, value_descriptor=RangeParameter(1, 15), name=\"Insulation Resistance\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# Add window-to-wall ratio as a parameter between 0.1 and 0.9 using a custom function\n",
    "parameters.append(wwr(RangeParameter(0.1, 0.9)))\n",
    "\n",
    "\n",
    "# Construct the objective\n",
    "objective = [\"Electricity:Facility\"]\n",
    "\n",
    "\n",
    "# Build the problem\n",
    "problem = EPProblem(parameters, objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the evaluator\n",
    "evaluator = EvaluatorEP(\n",
    "    problem,\n",
    "    building,\n",
    "    epw_file=\"victoria.epw\",\n",
    "    multi=True,\n",
    "    progress_bar=True,\n",
    "    distributed=True,\n",
    "    out_dir=\"outputdirectory\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Generate the Dataset\n",
    "\n",
    "1. Sample the problem space\n",
    "2. Setup the parallel processing\n",
    "3. Generate the Samples\n",
    "4. Store and recover the expensive runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use latin hypercube sampling to take 30 samples\n",
    "inputs = sampling.dist_sampler(sampling.lhs, problem, 100)\n",
    "\n",
    "\n",
    "# sample of the inputs\n",
    "print(inputs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parallel processing in the notebook.\n",
    "client = Client(threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "# Run Energyplus\n",
    "outputs = evaluator.df_apply(inputs)\n",
    "t2 = time.time()\n",
    "time_of_sim = t2 - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def niceformat(seconds):\n",
    "    seconds = seconds % (24 * 3600)\n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hour, minutes, seconds\n",
    "\n",
    "\n",
    "hours, mins, secs = niceformat(time_of_sim)\n",
    "\n",
    "print(\n",
    "    \"The total running time: {:2.0f} hours {:2.0f} min {:2.0f} seconds\".format(\n",
    "        hours, mins, secs\n",
    "    )\n",
    ")\n",
    "# Build a results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = inputs.join(outputs)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_heating_use = results[\"Electricity:Facility\"]\n",
    "\n",
    "\n",
    "def norm_res(results):\n",
    "    results_normed = (results - np.mean(results)) / np.std(results)\n",
    "    return results_normed\n",
    "\n",
    "\n",
    "plt.scatter(\n",
    "    norm_res(results[\"Solar Gain Coefficient\"]), total_heating_use, label=\"solar gain\"\n",
    ")\n",
    "plt.scatter(\n",
    "    norm_res(results[\"Window to Wall Ratio\"]), total_heating_use, label=\"w2w ratio\"\n",
    ")\n",
    "plt.scatter(\n",
    "    norm_res(results[\"Insulation Resistance\"]),\n",
    "    total_heating_use,\n",
    "    label=\"Insulation Resistance\",\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the expensive calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this can quite a big run. Lets store the results such that we don't have to rerun this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.to_pickle(\"inputs.pkl\")\n",
    "outputs.to_pickle(\"outputs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ = pd.read_pickle(\"inputs.pkl\")\n",
    "outputs_ = pd.read_pickle(\"outputs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) Setup the dataset for the Surrogate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are packed in a single columns which will not work for tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs_.head())\n",
    "print(inputs_.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repack them using the following code, to get 365 different columns which will represent the output labels. Build the full dataset with inputs and outputs to easily split up the train and test data sets. The training data sets are used to train the model, while the test data set will show how general the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = inputs_.join(outputs_)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "training_labels = train_dataset[outputs_.columns]\n",
    "testing_labels = test_dataset[outputs_.columns]\n",
    "training_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the Data (Inputs of the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize the inputs and the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train_dataset[inputs_.columns]\n",
    "train_stats = train_stats.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use the stats we calculated to do the normalization on the input.\n",
    "def norm_input(x):\n",
    "    return (x - train_stats[\"mean\"]) / train_stats[\"std\"]\n",
    "\n",
    "\n",
    "def unnorm_input(x):\n",
    "    return (x * train_stats[\"std\"]) + train_stats[\"mean\"]\n",
    "\n",
    "\n",
    "normed_train_data = norm_input(train_dataset[inputs_.columns])\n",
    "normed_test_data = norm_input(test_dataset[inputs_.columns])\n",
    "\n",
    "\n",
    "print(test_dataset[inputs_.columns].head())\n",
    "print(normed_test_data.head())\n",
    "print(unnorm_input(normed_test_data.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the labels (Outputs of the model)\n",
    "\n",
    "labels are the actual outputs that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(training_labels)\n",
    "train_std = np.std(testing_labels)\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "def norm_output(x):\n",
    "    return (x - train_mean) / train_std\n",
    "\n",
    "\n",
    "def unnorm_output(x):\n",
    "    return (x * train_std) + train_mean\n",
    "\n",
    "\n",
    "train_labels = norm_output(training_labels)\n",
    "test_labels = norm_output(testing_labels)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# (5) Build & Train Surrogate model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(5, input_shape=[len(train_dataset[inputs_.columns].keys())]),\n",
    "            layers.Dense(5),\n",
    "            layers.Dense(1),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.0001)\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\", \"mse\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(\n",
    "    normed_train_data,\n",
    "    train_labels,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    "    callbacks=[tfdocs.modeling.EpochDots()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot({\"Basic\": history}, metric=\"loss\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (6) Surrogate Model & Validate against the Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See -> https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "# R squared score:\n",
    "r_sqared_scores = []\n",
    "sum_res_s = []\n",
    "sum_tot_s = []\n",
    "y_i = test_labels.loc[test_labels.index].values\n",
    "y_m = np.mean(y_i) / y_i.size\n",
    "for i in range(len(normed_test_data)):\n",
    "    x_i = normed_test_data.loc[normed_test_data.index[i]].tolist()\n",
    "    f_i = model.predict([x_i])[0]\n",
    "    y_i = test_labels.loc[test_labels.index[i]].values\n",
    "    ss_res = (f_i - y_i) ** 2\n",
    "    ss_tot = (y_i - y_m) ** 2\n",
    "    sum_res_s.append(f_i)\n",
    "    sum_tot_s.append(y_i)\n",
    "    r_sqared_scores.append(1 - ss_res / ss_tot)\n",
    "\n",
    "plt.scatter(sum_res_s, sum_tot_s)\n",
    "plt.xlabel(\"predicted values\")\n",
    "plt.ylabel(\"test values\")\n",
    "print(\"average R sqaured score: {}\".format(np.mean(r_sqared_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# (7) Sample Surrogate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_func(ind):\n",
    "    vals = norm_input(list(ind))\n",
    "    output = unnorm_output(model.predict([list(vals)])[0][0])\n",
    "    return ((output.values[0],), ())\n",
    "\n",
    "\n",
    "GP_SM = EvaluatorGeneric(evaluation_func, problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srinputs = sampling.dist_sampler(sampling.lhs, problem, 100)\n",
    "sroutputs = GP_SM.df_apply(srinputs)\n",
    "srresults = srinputs.join(sroutputs)\n",
    "srresults.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (8) Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.parallel_coordinates(\n",
    "    srresults,\n",
    "    color=\"Electricity:Facility\",\n",
    "    dimensions=[\n",
    "        \"Window to Wall Ratio\",\n",
    "        \"Insulation Resistance\",\n",
    "        \"Solar Gain Coefficient\",\n",
    "        \"Electricity:Facility\",\n",
    "    ],\n",
    "    color_continuous_scale=px.colors.diverging.Tealrose,\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
