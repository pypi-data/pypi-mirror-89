# -*- coding: utf-8 -*-

"""Automated Tool for Optimized Modelling (ATOM).

Author: tvdboom
Description: Module containing the plotting classes.

"""

# Standard packages
import random
import numpy as np
import pandas as pd
from itertools import chain
from inspect import signature
from collections import defaultdict
from typeguard import typechecked
from joblib import Parallel, delayed
from contextlib import contextmanager
from scipy.stats.mstats import mquantiles
from typing import Optional, Union, Tuple

# Plotting packages
import shap
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.transforms import blended_transform_factory
from matplotlib.ticker import MaxNLocator
from matplotlib.patches import ConnectionStyle
from mpl_toolkits.axes_grid1 import make_axes_locatable
from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec

# Sklearn
from sklearn.utils import _safe_indexing
from sklearn.inspection import permutation_importance
from sklearn.calibration import calibration_curve
from sklearn.metrics import SCORERS, roc_curve, precision_recall_curve

# Own modules
from atom.basetransformer import BaseTransformer
from .utils import (
    SEQUENCE_TYPES, SCALAR, METRIC_ACRONYMS, flt, lst, check_is_fitted,
    check_dim, check_goal, check_binary_task, check_predict_proba,
    get_best_score, partial_dependence, composed, crash, plot_from_model,
)


class BaseFigure(object):
    """Class that stores the position of the current axes in grid.

    Parameters
    ----------
    nrows: int
        Number of subplot rows in the canvas.

    ncols: int
        Number of subplot columns in the canvas.

    """

    def __init__(self, nrows=1, ncols=1, is_canvas=False):
        self.nrows = nrows
        self.ncols = ncols
        self.is_canvas = is_canvas
        self._idx = -1

        # Create new figure and corresponding grid
        figure = plt.figure(constrained_layout=True if is_canvas else False)
        self.gridspec = GridSpec(nrows=self.nrows, ncols=self.ncols, figure=figure)

    @property
    def figure(self):
        """Get the current figure and increase the subplot index."""
        self._idx += 1

        # Check if there are too many plots in the contextmanager
        if self._idx >= self.nrows * self.ncols:
            raise RuntimeError(
                "Invalid number of plots in the canvas! Increase "
                "the number of rows and cols to add more plots."
            )

        return plt.gcf()

    @property
    def grid(self):
        """Return the position of the current axes in the grid."""
        return self.gridspec[self._idx]


class BasePlotter(object):
    """Parent class for all plotting methods.

    This base class defines the plot properties that can be changed in
    order to customize the plot's aesthetics. Making the variable
    mutable ensures that it is changed for all classes at the same time.

    """

    _fig = None
    _aesthetics = dict(
        style="darkgrid",  # Seaborn plotting style
        palette="GnBu_r_d",  # Matplotlib color palette
        title_fontsize=20,  # Fontsize for titles
        label_fontsize=16,  # Fontsize for labels and legends
        tick_fontsize=12,  # Fontsize for ticks
    )
    sns.set_style(_aesthetics["style"])
    sns.set_palette(_aesthetics["palette"])

    # Properties =================================================== >>

    @property
    def aesthetics(self):
        return self._aesthetics

    @aesthetics.setter
    @typechecked
    def aesthetics(self, aesthetics: dict):
        self.style = aesthetics.get("style", self.style)
        self.palette = aesthetics.get("palette", self.palette)
        self.title_fontsize = aesthetics.get("title_fontsize", self.title_fontsize)
        self.label_fontsize = aesthetics.get("label_fontsize", self.label_fontsize)
        self.tick_fontsize = aesthetics.get("tick_fontsize", self.tick_fontsize)

    @property
    def style(self):
        return self._aesthetics["style"]

    @style.setter
    @typechecked
    def style(self, style: str):
        styles = ["darkgrid", "whitegrid", "dark", "white", "ticks"]
        if style not in styles:
            raise ValueError(
                "Invalid value for the style parameter, got "
                f"{style}. Choose from {', '.join(styles)}."
            )
        sns.set_style(style)
        self._aesthetics["style"] = style

    @property
    def palette(self):
        return self._aesthetics["palette"]

    @palette.setter
    @typechecked
    def palette(self, palette: str):
        sns.set_palette(palette)
        self._aesthetics["palette"] = palette

    @property
    def title_fontsize(self):
        return self._aesthetics["title_fontsize"]

    @title_fontsize.setter
    @typechecked
    def title_fontsize(self, title_fontsize: int):
        if title_fontsize <= 0:
            raise ValueError(
                "Invalid value for the title_fontsize parameter. "
                f"Value should be >=0, got {title_fontsize}."
            )
        self._aesthetics["title_fontsize"] = title_fontsize

    @property
    def label_fontsize(self):
        return self._aesthetics["label_fontsize"]

    @label_fontsize.setter
    @typechecked
    def label_fontsize(self, label_fontsize: int):
        if label_fontsize <= 0:
            raise ValueError(
                "Invalid value for the label_fontsize parameter. "
                f"Value should be >=0, got {label_fontsize}."
            )
        self._aesthetics["label_fontsize"] = label_fontsize

    @property
    def tick_fontsize(self):
        return self._aesthetics["tick_fontsize"]

    @tick_fontsize.setter
    @typechecked
    def tick_fontsize(self, tick_fontsize: int):
        if tick_fontsize <= 0:
            raise ValueError(
                "Invalid value for the tick_fontsize parameter. "
                f"Value should be >=0, got {tick_fontsize}."
            )
        self._aesthetics["tick_fontsize"] = tick_fontsize

    def reset_aesthetics(self):
        """Reset the plot aesthetics to their default values."""
        self.aesthetics = dict(
            style="darkgrid",
            palette="GnBu_r_d",
            title_fontsize=20,
            label_fontsize=16,
            tick_fontsize=12,
        )

    # Methods ====================================================== >>

    @staticmethod
    def _get_figure():
        """Return existing figure if in canvas, else a new figure."""
        if BasePlotter._fig and BasePlotter._fig.is_canvas:
            return BasePlotter._fig.figure
        else:
            BasePlotter._fig = BaseFigure()
            return BasePlotter._fig.figure

    def _get_subclass(self, models, max_one=False):
        """Check and return the provided parameter models.

        Parameters
        ----------
        models: str or sequence
            Models provided by the plot's parameter.

        max_one: bool, optional (default=False)
            Whether one or multiple models are allowed. If True, return
            the model instead of a list.

        """
        models = self._get_models(models)
        model_subclasses = [m for m in self.models_ if m.name in models]

        if max_one and len(model_subclasses) > 1:
            raise ValueError("This plot method allows only one model at a time!")

        return model_subclasses[0] if max_one else model_subclasses

    def _get_metric(self, metric):
        """Check and return the provided parameter metric."""
        if isinstance(metric, str):
            if metric.lower() in METRIC_ACRONYMS:
                metric = METRIC_ACRONYMS[metric]

            # Return index corresponding to str
            for i, m in enumerate(self.metric_):
                if metric.lower() == m.name:
                    return i

        # If index in available metrics, return as provided
        elif 0 <= metric < len(self.metric_):
            return metric

        # If no match was found, raise an exception
        raise ValueError(
            "Invalid value for the metric parameter. Value should be the index"
            f" or name of a metric used to run the pipeline, got {metric}."
        )

    @staticmethod
    def _get_set(dataset):
        """Check and return the provided parameter metric."""
        if dataset.lower() == "both":
            return ["train", "test"]
        elif dataset.lower() in ("train", "test"):
            return [dataset.lower()]
        else:
            raise ValueError(
                "Invalid value for the dataset parameter. "
                "Choose between 'train', 'test' or 'both'."
            )

    def _get_show(self, show):
        """Check and return the provided parameter show."""
        max_fxs = max([len(branch.features) for branch in self._branches.values()])
        if show is None or show > max_fxs:
            return max_fxs
        elif show < 1:
            raise ValueError(
                "Invalid value for the show parameter."
                f"Value should be >0, got {show}."
            )

        return show

    @staticmethod
    def _get_index(index, model):
        """Check and return the provided parameter index."""
        if index is None:
            rows = model.X_test
        elif isinstance(index, int):
            if index < 0:
                rows = model.X.iloc[[len(model.X) + index]]
            else:
                rows = model.X.iloc[[index]]
        else:
            rows = model.X.iloc[slice(*index)]

        if rows.empty:
            raise ValueError(
                "Invalid value for parameter index. Couldn't find "
                f"the specified rows in X, got index: {index}."
            )

        return rows

    def _get_target(self, target):
        """Check and return the provided parameter target."""
        if self.task.startswith("reg"):
            return None, None

        if isinstance(target, str):
            if target not in self.mapping:
                raise ValueError(
                    f"Invalid value for the target parameter. Value {target} "
                    "not found in the mapping of the target column."
                )
            tgt_int = self.mapping[target]
            return tgt_int, target

        else:
            if not 0 <= target < self.n_classes:
                raise ValueError(
                    "Invalid value for the target parameter. There are "
                    f"{self.n_classes} classes, got {target}."
                )
            # Get mapping key from value
            tgt_str = list(self.mapping)[list(self.mapping.values()).index(target)]
            return target, tgt_str

    @staticmethod
    def _get_shap(model, data, target):
        """Get the SHAP information of a model.

        Parameters
        ----------
        model: class
         Model subclass.

        data: pd.DataFrame
            Data on which to calculate the shap values.

        target: int
            Index of the target class in the target column.

        Returns
        -------
        shap_values: np.ndarray
            SHAP values for the target class.

        expected_value: float
            Difference between the model output for that sample and
            the expected value of the model output.

        """
        # TODO: Refactor to shap's newest API
        if model.type == "tree":
            explainer = shap.TreeExplainer(
                model.estimator, feature_perturbation="tree_path_dependent"
            )
        elif model.type == "linear":
            explainer = shap.LinearExplainer(model.estimator, model.X_train)
        else:
            if len(model.X_train) <= 100:
                k_data = model.X_train
            else:
                k_data = shap.kmeans(model.X_train, 100)
            explainer = shap.KernelExplainer(model.estimator.predict, k_data)

        shap_values = explainer.shap_values(data)
        expected_value = explainer.expected_value

        # Select the values corresponding to the target
        if not np.array(shap_values).shape == (len(data), model.X.shape[1]):
            shap_values = shap_values[target]

        # Select the target expected value or return all
        if isinstance(expected_value, (list, np.ndarray)):
            if len(expected_value) == model.n_classes:
                expected_value = expected_value[target]

        return shap_values, expected_value

    def _plot(self, ax=None, **kwargs):
        """Make the plot.

        Customize the axes to the default layout and plot the figure
        if it's not part of a canvas.

        Parameters
        ----------
        ax: matplotlib.axes.Axes or None, optional (default=None)
            Current plotting axes. If None, ignore the axes.

        **kwargs
            Keyword arguments containing the plot's parameters.
            Axes parameters:
                - title: Axes' title.
                - legend: Location of the legend and number of items.
                - xlabel: Label for the x-axis.
                - ylabel: Label for the y-axis.
                - xlim: Limits for the x-axis.
                - ylim: Limits for the y-axis.
            Figure parameters:
                - figsize: Size of the figure.
                - tight_layout: Whether to apply it (default=True).
                - filename: Name of the saved file.
                - display: Whether to render the plot.

        """
        if kwargs.get("title"):
            ax.set_title(kwargs.get("title"), fontsize=self.title_fontsize, pad=20)
        if kwargs.get("legend"):
            ax.legend(
                loc=kwargs["legend"][0],
                ncol=kwargs["legend"][1] // 3 if kwargs["legend"][1] // 3 > 0 else 1,
                fontsize=self.label_fontsize
            )
        if kwargs.get("xlabel"):
            ax.set_xlabel(kwargs["xlabel"], fontsize=self.label_fontsize, labelpad=12)
        if kwargs.get("ylabel"):
            ax.set_ylabel(kwargs["ylabel"], fontsize=self.label_fontsize, labelpad=12)
        if kwargs.get("xlim"):
            ax.set_xlim(kwargs["xlim"])
        if kwargs.get("ylim"):
            ax.set_ylim(kwargs["ylim"])
        if ax is not None:
            ax.tick_params(axis='both', labelsize=self.tick_fontsize)

        if not getattr(BasePlotter._fig, "is_canvas", None):
            if kwargs.get("figsize"):
                plt.gcf().set_size_inches(*kwargs["figsize"])
            if kwargs.get("tight_layout", True):
                plt.tight_layout()
            if kwargs.get("filename"):
                plt.savefig(kwargs["filename"])
            if "filename" in kwargs:
                plt.show() if kwargs.get("display") else plt.close()

    @composed(contextmanager, crash, typechecked)
    def canvas(
        self,
        nrows: int = 1,
        ncols: int = 2,
        title: Optional[str] = None,
        figsize: Optional[Tuple[SCALAR, SCALAR]] = None,
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Create a figure with multiple plots.

        This `@contextmanager` allows you to draw many plots in one
        figure. The default option is to add two plots side by side.

        Parameters
        ----------
        nrows: int, optional (default=1)
            Number of plots in length.

        ncols: int, optional (default=2)
            Number of plots in width.

        title: str or None, optional (default=None)
            Plot's title. If None, no title is displayed.

        figsize: tuple or None, optional (default=None)
            Figure's size, format as (x, y). If None, adapts size to
            the number of plots in the canvas.

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        BasePlotter._fig = BaseFigure(nrows=nrows, ncols=ncols, is_canvas=True)
        try:
            yield self
        finally:
            if title:
                plt.suptitle(title, fontsize=self.title_fontsize + 4)
            BasePlotter._fig.is_canvas = False  # Close the canvas
            self._plot(
                figsize=figsize if figsize else (6 + 4 * ncols, 2 + 4 * nrows),
                tight_layout=False,
                filename=filename,
                display=display
            )


class FSPlotter(BasePlotter):
    """Plots for the FeatureSelector class."""

    @composed(crash, typechecked)
    def plot_pca(
        self,
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the explained variance ratio vs number of components.

        Parameters
        ----------
        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        if not hasattr(self.branch, "pca") or not self.pca:
            raise PermissionError(
                "The plot_pca method is only available if PCA was applied on the data!"
            )

        n = self.pca.n_components_  # Number of chosen components
        var = np.array(self.pca.explained_variance_ratio_[:n])
        var_all = np.array(self.pca.explained_variance_ratio_)

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        ax.scatter(
            x=self.pca.n_components_ - 1,
            y=var.sum(),
            marker="*",
            s=130,
            c="blue",
            edgecolors="b",
            zorder=3,
            label=f"Total variance retained: {round(var.sum(), 3)}",
        )
        ax.plot(range(self.pca.n_features_), np.cumsum(var_all), marker="o")
        ax.axhline(var.sum(), ls="--", color="k")
        ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Only int ticks

        self._plot(
            ax=ax,
            title="PCA explained variances" if title is None else title,
            legend=("lower right", 1),
            xlabel="First N principal components",
            ylabel="Cumulative variance ratio",
            figsize=figsize,
            filename=filename,
            display=display,
        )

    @composed(crash, typechecked)
    def plot_components(
        self,
        show: Optional[int] = None,
        title: Optional[str] = None,
        figsize: Optional[Tuple[SCALAR, SCALAR]] = None,
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the explained variance ratio per component.

        Parameters
        ----------
        show: int or None, optional (default=None)
            Number of components to show. If None, the number of
            components in the data are plotted.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=None)
            Figure's size, format as (x, y). If None, adapts size
            to the number of components shown.

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        if not hasattr(self.branch, "pca") or not self.pca:
            raise PermissionError(
                "The plot_components method is only available "
                "if PCA was applied on the data!"
            )

        # Set parameters
        if show is None:
            show = self.pca.n_components_
        elif show > self.pca.n_features_:
            show = self.pca.n_features_
        elif show < 1:
            raise ValueError(
                "Invalid value for the show parameter. "
                f"Value should be >0, got {show}."
            )

        var = np.array(self.pca.explained_variance_ratio_)[:show]
        indices = [f"Component {str(i)}" for i in range(len(var))]
        scr = pd.Series(var, index=indices).sort_values()

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        scr.plot.barh(label=f"Total variance retained: {var.sum():.3f}", width=0.6)
        ax.set_xlim(0, max(scr) + 0.1 * max(scr))  # Make extra space for numbers
        for i, v in enumerate(scr):
            ax.text(v + 0.005, i - 0.08, f"{v:.3f}", fontsize=self.tick_fontsize)

        self._plot(
            ax=ax,
            title="Explained variance per component" if title is None else title,
            legend=("lower right", 1),
            xlabel="Explained variance ratio",
            figsize=figsize if figsize else (10, 4 + show // 2),
            filename=filename,
            display=display,
        )

    @composed(crash, typechecked)
    def plot_rfecv(
        self,
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the RFECV results.

        Plot the scores obtained by the estimator fitted on every
        subset of the dataset. Only if RFECV was applied on the
        dataset through the feature_engineering method.

        Parameters
        ----------
        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=None)
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        if not hasattr(self.branch, "rfecv") or not self.rfecv:
            raise PermissionError(
                "The plot_rfecv method is only available "
                "if RFECV was applied on the data!"
            )

        try:  # Define the y-label for the plot
            ylabel = self.rfecv.get_params()["scoring"].name
        except AttributeError:
            ylabel = "score"
            if self.rfecv.get_params()["scoring"]:
                ylabel = str(self.rfecv.get_params()["scoring"])

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        n_features = self.rfecv.get_params()["min_features_to_select"]
        xline = range(n_features, n_features + len(self.rfecv.grid_scores_))
        ax.plot(xline, self.rfecv.grid_scores_)

        # Set limits before drawing the intersected lines
        xlim = (n_features - 0.5, n_features + len(self.rfecv.grid_scores_) - 0.5)
        ylim = ax.get_ylim()

        # Draw intersected lines
        x = xline[np.argmax(self.rfecv.grid_scores_)]
        y = max(self.rfecv.grid_scores_)
        ax.vlines(x, -1e4, y, ls="--", color="k", alpha=0.7)
        ax.hlines(
            y=y,
            xmin=-1,
            xmax=x,
            ls="--",
            color="k",
            alpha=0.7,
            label=f"Features: {x}   {ylabel}: {round(y, 3)}",
        )
        ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Only int ticks

        self._plot(
            ax=ax,
            title="RFECV scores" if title is None else title,
            legend=("best", 1),
            xlabel="Number of features",
            ylabel=ylabel,
            xlim=xlim,
            ylim=ylim,
            figsize=figsize,
            filename=filename,
            display=display,
        )


class BaseModelPlotter(BasePlotter):
    """Plots for the BaseModel class."""

    @composed(crash, plot_from_model, typechecked)
    def plot_results(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        metric: Union[int, str] = 0,
        title: Optional[str] = None,
        figsize: Optional[Tuple[SCALAR, SCALAR]] = None,
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot of the model results after the evaluation.

        If all models applied bagging, the plot will be a boxplot.
        If not, the plot will be a barplot. Models are ordered based
        on their score from the top down. The score is either the
        `mean_bagging` or `metric_test` attribute of the model,
        selected in that order.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        metric: int or str, optional (default=0)
            Index or name of the metric. Only for multi-metric runs.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=None)
            Figure's size, format as (x, y). If None, adapts size to
            the number of models.

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        def get_bagging(m):
            """Get the bagging's results for a specific metric."""
            if getattr(m, "metric_bagging", None):
                if len(self.metric_) == 1:
                    return m.metric_bagging
                else:
                    return m.metric_bagging[metric]

        def std(m):
            """Get the standard deviation of the bagging's results."""
            if getattr(m, "std_bagging", None):
                return lst(m.std_bagging)[metric]
            else:
                return 0

        check_is_fitted(self, "results")
        models = self._get_subclass(models)
        metric = self._get_metric(metric)

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)

        names = []
        models = sorted(models, key=lambda m: get_best_score(m, metric))
        color = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]  # First color

        all_bagging = all(get_bagging(m) for m in models)
        for i, m in enumerate(models):
            names.append(m.name)
            if all_bagging:
                ax.boxplot(
                    x=get_bagging(m),
                    vert=False,
                    positions=[i],
                    widths=0.5,
                    boxprops=dict(color=color)
                )
            else:
                ax.barh(
                    y=i,
                    width=get_best_score(m, metric),
                    height=0.5,
                    xerr=std(m),
                    color=color
                )

        min_lim = 0.95 * (get_best_score(models[0], metric) - std(models[0]))
        max_lim = 1.01 * (get_best_score(models[-1], metric) + std(models[-1]))
        ax.set_yticks(range(len(models)))
        ax.set_yticklabels(names)
        self._plot(
            ax=ax,
            title="Model evaluation" if title is None else title,
            xlabel=self.metric_[metric].name,
            xlim=(min_lim, max_lim) if not all_bagging else None,
            figsize=figsize if figsize else (10, 4 + len(models) // 2),
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_bo(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        metric: Union[int, str] = 0,
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 8),
        filename: Optional[str] = None,
        display: bool = True,
    ):

        """Plot the bayesian optimization scoring.

        Only for models that ran the hyperparameter optimization. This
        is the same plot as produced by `bo_params={"plot_bo": True}`
        while running the BO. Creates a canvas with two plots: the
        first plot shows the score of every trial and the second shows
        the distance between the last consecutive steps.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline that used bayesian optimization are selected.

        metric: int or str, optional (default=0)
            Index or name of the metric. Only for multi-metric runs.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 8))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        models = self._get_subclass(models)
        metric = self._get_metric(metric)

        # Check there is at least one model that run the BO
        if all([m.bo.empty for m in models]):
            raise PermissionError(
                "The plot_bo method is only available for models that "
                "ran the bayesian optimization hyperparameter tuning!"
            )

        fig = self._get_figure()
        gs = GridSpecFromSubplotSpec(4, 1, BasePlotter._fig.grid, hspace=0.05)
        ax1 = fig.add_subplot(gs[0:3, 0])
        ax2 = plt.subplot(gs[3:4, 0], sharex=ax1)
        for m in models:
            y = m.bo["score"].apply(lambda value: lst(value)[metric])
            if len(models) == 1:
                label = f"Score={round(lst(m.metric_bo)[metric], 3)}"
            else:
                label = f"{m.name} (Score={round(lst(m.metric_bo)[metric], 3)})"

            # Draw bullets on all markers except the maximum
            markers = [i for i in range(len(m.bo))]
            markers.remove(int(np.argmax(y)))
            ax1.plot(range(1, len(y) + 1), y, "-o", markevery=markers, label=label)
            ax2.plot(range(2, len(y) + 1), np.abs(np.diff(y)), "-o")
            ax1.scatter(np.argmax(y) + 1, max(y), zorder=10, s=100, marker="*")

        plt.setp(ax1.get_xticklabels(), visible=False)
        self._plot(
            ax=ax1,
            title="Bayesian optimization performance" if title is None else title,
            legend=("lower right", len(models)),
            ylabel=self.metric_[metric].name,
        )
        ax2.xaxis.set_major_locator(MaxNLocator(integer=True))
        self._plot(
            ax=ax2,
            xlabel="Call",
            ylabel="d",
            figsize=figsize,
            filename=filename,
            display=display
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_evals(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        dataset: str = "both",
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot evaluation curves for the train and test set.

         Only for models that allow in-training evaluation (XGB, LGB,
        CastB). The metric is provided by the estimator's package and
        is different for every model and every task. For this reason,
        the method only allows plotting one model at a time.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the model to plot. If None, all models in the
            pipeline are selected. Note that leaving the default
            option could raise an exception if there are multiple
            models in the pipeline. To avoid this, call the plot
            from a model, e.g. `atom.lgb.plot_evals()`.

        dataset: str, optional (default="both")
            Data set on which to calculate the evaluation curves.
            Options are "train", "test" or "both".

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_dim(self, "plot_evals")
        check_is_fitted(self, "results")
        m = self._get_subclass(models, max_one=True)
        dataset = self._get_set(dataset)

        # Check that the model had in-training evaluation
        if not hasattr(m, "evals"):
            raise AttributeError(
                "The plot_evals method is only available for models "
                f"that allow in-training evaluation, got {m.name}."
            )

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        for set_ in dataset:
            ax.plot(range(len(m.evals[set_])), m.evals[set_], lw=2, label=set_)

        self._plot(
            ax=ax,
            title="Evaluation curves" if title is None else title,
            legend=("best", len(dataset)),
            xlabel=m.get_dimensions()[0].name,  # First param is always the iter
            ylabel=m.evals["metric"],
            figsize=figsize,
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_roc(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        dataset: str = "test",
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the Receiver Operating Characteristics curve.

        The legend shows the Area Under the ROC Curve (AUC) score.
        Only for binary classification tasks.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        dataset: str, optional (default="test")
            Data set on which to calculate the metric. Options are
            "train", "test" or "both".

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10,6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        check_binary_task(self, "plot_roc")
        models = self._get_subclass(models)
        dataset = self._get_set(dataset)
        check_predict_proba(models, "plot_roc")

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        for m in models:
            for set_ in dataset:
                # Get False (True) Positive Rate as arrays
                fpr, tpr, _ = roc_curve(
                    getattr(m, f"y_{set_}"), getattr(m, f"predict_proba_{set_}")[:, 1]
                )

                if len(models) == 1:
                    l_set = f"{set_} - " if len(dataset) > 1 else ""
                    label = f"{l_set}AUC={m.scoring('roc_auc', set_):.3f}"
                else:
                    l_set = f" - {set_}" if len(dataset) > 1 else ""
                    label = f"{m.name}{l_set} (AUC={m.scoring('roc_auc', set_):.3f})"
                ax.plot(fpr, tpr, lw=2, label=label)

        ax.plot([0, 1], [0, 1], lw=2, color="black", alpha=0.7, linestyle="--")

        self._plot(
            ax=ax,
            title="ROC curve" if title is None else title,
            legend=("lower right", len(models)),
            xlabel="FPR",
            ylabel="TPR",
            figsize=figsize,
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_prc(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        dataset: str = "test",
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the precision-recall curve.

        The legend shows the average precision (AP) score. Only for
        binary classification tasks.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        dataset: str, optional (default="test")
            Data set on which to calculate the metric. Options are
            "train", "test" or "both".

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        check_binary_task(self, "plot_prc")
        models = self._get_subclass(models)
        dataset = self._get_set(dataset)
        check_predict_proba(models, "plot_prc")

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        for m in models:
            for set_ in dataset:
                # Get precision-recall pairs for different thresholds
                precision, recall, _ = precision_recall_curve(
                    getattr(m, f"y_{set_}"), getattr(m, f"predict_proba_{set_}")[:, 1]
                )

                if len(models) == 1:
                    l_set = f"{set_} - " if len(dataset) > 1 else ""
                    label = f"{l_set}AP={m.scoring('ap', set_):.3f}"
                else:
                    l_set = f" - {set_}" if len(dataset) > 1 else ""
                    label = f"{m.name}{l_set} (AP={m.scoring('ap', set_):.3f})"
                plt.plot(recall, precision, lw=2, label=label)

        self._plot(
            ax=ax,
            title="Precision-recall curve" if title is None else title,
            legend=("lower left", len(models)),
            xlabel="Recall",
            ylabel="Precision",
            figsize=figsize,
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_permutation_importance(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        show: Optional[int] = None,
        n_repeats: int = 10,
        title: Optional[str] = None,
        figsize: Optional[Tuple[SCALAR, SCALAR]] = None,
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the feature permutation importance of models.

        If a permutation is repeated for the same model with the
        same amount of n_repeats, the calculation is skipped. The
        `feature_importance` attribute is updated with the extracted
        importance ranking.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        show: int or None, optional (default=None)
            Number of best features to show. None to show all.

        n_repeats: int, optional (default=10)
            Number of times to permute each feature.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple or None, optional (default=None)
            Figure's size, format as (x, y). If None, adapts size
            to the number of features shown.

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_dim(self, "plot_permutation_importance")
        check_is_fitted(self, "results")
        models = self._get_subclass(models)
        show = self._get_show(show)

        if n_repeats <= 0:
            raise ValueError(
                "Invalid value for the n_repeats parameter."
                f"Value should be >0, got {n_repeats}."
            )

        # Create dataframe with columns as indices to plot with barh
        df = pd.DataFrame(columns=["features", "score", "model"])

        # Create dictionary to store the permutations per model
        if not hasattr(self, "permutations"):
            self.permutations = {}

        for m in models:
            # If permutations are already calculated and n_repeats is
            # same, use known permutations (for efficient re-plotting)
            if not hasattr(m, "_repts"):
                m._repeats = -np.inf
            if m.name not in self.permutations or m._repeats != n_repeats:
                m._repeats = n_repeats
                # Permutation importances returns Bunch object
                self.permutations[m.name] = permutation_importance(
                    estimator=m.estimator,
                    X=m.X_test,
                    y=m.y_test,
                    scoring=self.metric_[0],
                    n_repeats=n_repeats,
                    n_jobs=self.n_jobs,
                    random_state=self.random_state,
                )

            # Append data to the dataframe
            for i, feature in enumerate(m.features):
                for score in self.permutations[m.name].importances[i, :]:
                    df = df.append({
                        "features": feature,
                        "score": score,
                        "model": m.name
                    }, ignore_index=True)

        # Get the column names sorted by sum of scores
        get_idx = df.groupby("features", as_index=False)["score"].sum()
        get_idx = get_idx.sort_values("score", ascending=False)
        column_order = get_idx.features.values[:show]

        # Save the best feature order
        self.branch.feature_importance = list(get_idx.features.values)

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        sns.boxplot(
            x="score",
            y="features",
            hue="model",
            data=df,
            ax=ax,
            order=column_order,
            width=0.75 if len(models) > 1 else 0.6,
        )

        ax.yaxis.label.set_visible(False)
        if len(models) > 1:
            # Remove seaborn's legend title
            handles, labels = ax.get_legend_handles_labels()
            ax.legend(handles=handles[1:], labels=labels[1:])
        else:
            # Hide the legend created by seaborn
            ax.legend().set_visible(False)

        self._plot(
            ax=ax,
            title="Permutation importance" if title is None else title,
            legend=("lower right" if len(models) > 1 else False, len(models)),
            xlabel="Score",
            figsize=figsize if figsize else (10, 4 + show // 2),
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_feature_importance(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        show: Optional[int] = None,
        title: Optional[str] = None,
        figsize: Optional[Tuple[SCALAR, SCALAR]] = None,
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot a tree-based model's feature importance.

        The importances are normalized in order to be able to compare
        them between models. The `feature_importance` attribute is
        updated with the extracted importance ranking.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        show: int or None, optional (default=None)
            Number of best features to show in the plot. None to
            show all.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple or None, optional (default=None)
            Figure's size, format as (x, y). If None, adapts size
            to the number of features shown.

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_dim(self, "plot_feature_importance")
        check_is_fitted(self, "results")
        models = self._get_subclass(models)
        show = self._get_show(show)

        # Create dataframe with columns as indices to plot with barh
        df = pd.DataFrame()

        for m in models:
            # Bagging is a special case where we use the feature_importance per est
            if not hasattr(m.estimator, "feature_importances_") and m.acronym != "Bag":
                raise PermissionError(
                    "The plot_feature_importance method is only available for "
                    f"models with a feature_importances_ attribute, got {m.name}."
                )

            # Bagging has no direct feature importance implementation
            if m.acronym == "Bag":
                feature_importances = np.mean(
                    [fi.feature_importances_ for fi in m.estimator.estimators_], axis=0
                )
            else:
                feature_importances = m.estimator.feature_importances_

            # Normalize for plotting values adjacent to bar
            max_feature_importance = max(feature_importances)
            for col, fx in zip(m.features, feature_importances):
                df.at[col, m.name] = fx / max_feature_importance

        # Save the best feature order
        best_fxs = df.fillna(0).sort_values(by=df.columns[-1], ascending=False)
        self.branch.feature_importance = list(best_fxs.index.values)

        # Select best and sort ascending (by sum of total importances)
        df = df.nlargest(show, columns=df.columns[-1])
        df = df.reindex(sorted(df.index, key=lambda i: df.loc[i].sum()))

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        df.plot.barh(
            ax=ax,
            width=0.75 if len(models) > 1 else 0.6,
            legend=True if len(models) > 1 else False
        )
        if len(models) == 1:
            for i, v in enumerate(df[df.columns[0]]):
                ax.text(v + 0.01, i - 0.08, f"{v:.2f}", fontsize=self.tick_fontsize)

        self._plot(
            ax=ax,
            title="Normalized feature importance" if title is None else title,
            legend=("lower right" if len(models) > 1 else False, len(models)),
            xlim=(0, 1.03 if len(models) > 1 else 1.09),
            xlabel="Score",
            figsize=figsize if figsize else (10, 4 + show // 2),
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_partial_dependence(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        features: Optional[Union[int, str, SEQUENCE_TYPES]] = None,
        target: Union[int, str] = 1,
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the partial dependence of features.

        The partial dependence of a feature (or a set of features)
        corresponds to the average response of the model for each
        possible value of the feature. Two-way partial dependence
        plots are plotted as contour plots (only allowed for single
        model plots). The deciles of the feature values will be shown
        with tick marks on the x-axes for one-way plots, and on both
        axes for two-way plots.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        features: int, str, sequence or None, optional (default=None)
            Features or feature pairs (name or index) to get the partial
            dependence from. Maximum of 3 allowed. If None, it uses the
            best 3 features if the `feature_importance` attribute is
            defined, else it uses the first 3 features in the dataset.

        target: int or str, optional (default=1)
            Index or name of the class in the target column to look at.
            Only for multi-class classification tasks.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        def convert_feature(feature):
            if isinstance(feature, str):
                try:
                    feature = list(m.features).index(feature)
                except ValueError:
                    raise ValueError(
                        "Invalid value for the features parameter. "
                        f"Unknown column: {feature}."
                    )
            elif feature > m.X.shape[1] - 1:  # -1 because of index 0
                raise ValueError(
                    "Invalid value for the features parameter. Dataset "
                    f"has {m.X.shape[1]} features, got index {feature}."
                )
            return int(feature)

        def get_features(features, m):
            # Default is to select the best or the first 3 features
            if not features:
                if not m.branch.feature_importance:
                    features = [0, 1, 2]
                else:
                    features = m.branch.feature_importance[:3]

            features = lst(features)
            if len(features) > 3:
                raise ValueError(
                    "Invalid value for the features parameter. "
                    f"Maximum 3 allowed, got {len(features)}."
                )

            # Convert features into a sequence of int tuples
            cols = []
            for fxs in features:
                if isinstance(fxs, (int, str)):
                    cols.append((convert_feature(fxs),))
                elif len(models) == 1:
                    if len(fxs) == 2:
                        cols.append(tuple(convert_feature(fx) for fx in fxs))
                    else:
                        raise ValueError(
                            "Invalid value for the features parameter. Values "
                            f"should be single or in pairs, got {fxs}."
                        )
                else:
                    raise ValueError(
                        "Invalid value for the features parameter. Feature pairs "
                        f"are invalid when plotting multiple models, got {fxs}."
                    )

            return cols

        check_dim(self, "plot_partial_dependence")
        check_is_fitted(self, "results")
        models = self._get_subclass(models)
        tgt_int, tgt_str = self._get_target(target)

        # Only for this plot, target index is always 0 if not multiclass
        if not self.task.startswith("multi"):
            tgt_int = 0

        axes = []
        fig = self._get_figure()
        n_cols = 3 if not features else len(lst(features))
        gs = GridSpecFromSubplotSpec(1, n_cols, BasePlotter._fig.grid)
        for i in range(n_cols):
            axes.append(fig.add_subplot(gs[0, i]))

        names = []  # Names of the features (to compare between models)
        for m in models:
            # Since every model can have different fxs, select them again
            cols = get_features(features, m)

            # Make sure the models use the same features
            if len(models) > 1:
                fxs_names = [m.features[col[0]] for col in cols]
                if not names:
                    names = fxs_names
                elif names != fxs_names:
                    raise ValueError(
                        "Invalid value for the features parameter. Not all models "
                        f"use the same features, got {names} and {fxs_names}."
                    )

            # Compute averaged predictions
            pd_results = Parallel(n_jobs=self.n_jobs)(
                delayed(partial_dependence)(m.estimator, m.X_test, col) for col in cols
            )

            # Get global min and max average predictions of PD grouped by plot type
            pdp_lim = {}
            for pred, values in pd_results:
                min_pd, max_pd = pred[tgt_int].min(), pred[tgt_int].max()
                old_min, old_max = pdp_lim.get(len(values), (min_pd, max_pd))
                pdp_lim[len(values)] = (min(min_pd, old_min), max(max_pd, old_max))

            deciles = {}
            for fx in chain.from_iterable(cols):
                if fx not in deciles:  # Skip if the feature is repeated
                    X_col = _safe_indexing(m.X_test, fx, axis=1)
                    deciles[fx] = mquantiles(X_col, prob=np.arange(0.1, 1.0, 0.1))

            for axi, fx, (pred, values) in zip(axes, cols, pd_results):
                # For both types: draw ticks on the horizontal axis
                trans = blended_transform_factory(axi.transData, axi.transAxes)
                axi.vlines(deciles[fx[0]], 0, 0.05, transform=trans, color="k")
                axi.ticklabel_format(axis="y", style="sci", scilimits=(-2, 2))
                self._plot(ax=axi, xlabel=m.columns[fx[0]])

                # Draw line or contour plot
                if len(values) == 1:
                    axi.plot(values[0], pred[tgt_int].ravel(), lw=2, label=m.name)
                else:
                    # Create contour levels for two-way plots
                    levels = np.linspace(pdp_lim[2][0], pdp_lim[2][1] + 1e-9, num=8)

                    # Draw contour plot
                    XX, YY = np.meshgrid(values[0], values[1])
                    Z = pred[tgt_int].T
                    CS = axi.contour(XX, YY, Z, levels=levels, linewidths=0.5)
                    axi.clabel(CS, fmt="%2.2f", colors="k", fontsize=10, inline=True)
                    axi.contourf(
                        XX, YY, Z,
                        levels=levels,
                        vmax=levels[-1],
                        vmin=levels[0],
                        alpha=0.75,
                    )

                    # Draw the ticks on the vertical axis
                    trans = blended_transform_factory(axi.transAxes, axi.transData)
                    axi.hlines(deciles[fx[1]], 0, 0.05, transform=trans, color="k")

                    self._plot(
                        ax=axi,
                        ylabel=m.columns[fx[1]],
                        xlim=(min(XX.flatten()), max(XX.flatten())),
                        ylim=(min(YY.flatten()), max(YY.flatten())),
                    )

        # Place y-label and legend on first non-contour plot
        for axi in axes:
            if not axi.get_ylabel():
                self._plot(
                    ax=axi,
                    ylabel="Score",
                    legend=("best" if len(models) > 1 else False, len(models))
                )
                break

        if title is None:
            title = "Partial dependence"
            if self.task.startswith("multi"):
                title += f" for {m.y.name}={tgt_str}"

        # Place title if not in canvas, else above first or middle image
        if len(cols) == 1 or (len(cols) == 2 and BasePlotter._fig.is_canvas):
            axes[0].set_title(title, fontsize=self.title_fontsize, pad=20)
        elif len(cols) == 3:
            axes[1].set_title(title, fontsize=self.title_fontsize, pad=20)
        elif not BasePlotter._fig.is_canvas:
            plt.suptitle(title, fontsize=self.title_fontsize)

        self._plot(figsize=figsize, filename=filename, display=display)

    @composed(crash, plot_from_model, typechecked)
    def plot_errors(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        dataset: str = "test",
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot a model's prediction errors.

        Plot the actual targets from a set against the predicted values
        generated by the regressor. A linear fit is made on the data.
        The gray, intersected line shows the identity line. This pot can
        be useful to detect noise or heteroscedasticity along a range of
        the target domain. Only for regression tasks.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        dataset: str, optional (default="test")
            Data set on which to calculate the errors. Options are
            "train", "test" or "both".

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        check_goal(self, "plot_errors", "regression")
        models = self._get_subclass(models)
        dataset = self._get_set(dataset)

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        for m in models:
            for set_ in dataset:
                if len(models) == 1:
                    l_set = f"{set_} - " if len(dataset) > 1 else ""
                    label = f"{l_set}R$^2$={m.scoring('r2', set_):.3f}"
                else:
                    l_set = f" - {set_}" if len(dataset) > 1 else ""
                    label = f"{m.name}{l_set} (R$^2$={m.scoring('r2', set_):.3f})"

                ax.scatter(
                    x=getattr(self, f"y_{set_}"),
                    y=getattr(m, f"predict_{set_}"),
                    alpha=0.8,
                    label=label,
                )

                # Fit the points using linear regression
                from .models import OrdinaryLeastSquares

                model = OrdinaryLeastSquares(self).get_estimator()
                model.fit(
                    X=np.array(getattr(self, f"y_{set_}")).reshape(-1, 1),
                    y=getattr(m, f"predict_{set_}"),
                )

                # Draw the fit
                x = np.linspace(*ax.get_xlim(), 100)
                ax.plot(x, model.predict(x[:, np.newaxis]), lw=2, alpha=1)

        # Get limits before drawing the identity line
        xlim, ylim = ax.get_xlim(), ax.get_ylim()

        # Draw identity line
        ax.plot(xlim, ylim, ls="--", lw=1, color="k", alpha=0.7)

        self._plot(
            ax=ax,
            title="Prediction errors" if title is None else title,
            legend=("upper left", len(models)),
            xlabel="True value",
            ylabel="Predicted value",
            xlim=xlim,
            ylim=ylim,
            figsize=figsize,
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_residuals(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        dataset: str = "test",
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot a model's residuals.

        The plot shows the residuals (difference between the predicted
        and the true value) on the vertical axis and the independent
        variable on the horizontal axis. The gray, intersected line
        shows the identity line. This plot can be useful to analyze the
        variance of the error of the regressor. If the points are
        randomly dispersed around the horizontal axis, a linear
        regression model is appropriate for the data; otherwise, a
        non-linear model is more appropriate. Only for regression tasks.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        dataset: str, optional (default="test")
            Data set on which to calculate the metric. Options are
            "train", "test" or "both".

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        check_goal(self, "plot_residuals", "regression")
        models = self._get_subclass(models)
        dataset = self._get_set(dataset)

        fig = self._get_figure()
        gs = GridSpecFromSubplotSpec(1, 4, BasePlotter._fig.grid, wspace=0.05)
        ax1 = fig.add_subplot(gs[0, :3])
        ax2 = fig.add_subplot(gs[0, 3:4])

        for m in models:
            for set_ in dataset:
                if len(models) == 1:
                    l_set = f"{set_} - " if len(dataset) > 1 else ""
                    label = f"{l_set}R$^2$={m.scoring('r2', set_):.3f}"
                else:
                    l_set = f" - {set_}" if len(dataset) > 1 else ""
                    label = f"{m.name}{l_set} (R$^2$={m.scoring('r2', set_):.3f})"

            res = np.subtract(getattr(m, f"predict_{set_}"), getattr(self, f"y_{set_}"))
            ax1.scatter(getattr(m, f"predict_{set_}"), res, alpha=0.7, label=label)
            ax2.hist(res, orientation="horizontal", histtype="step", linewidth=1.2)

        # Get limits before drawing the identity line
        xlim, ylim = ax1.get_xlim(), ax1.get_ylim()
        ax1.hlines(0, *xlim, ls="--", lw=1, color="k", alpha=0.8)  # Identity line

        ax2.set_yticklabels([])
        self._plot(ax=ax2, xlabel="Distribution")

        if title is None:
            title = "Residuals plot"
        if not BasePlotter._fig.is_canvas:
            plt.suptitle(title, fontsize=self.title_fontsize, y=0.98)
        else:
            ax1.set_title(title, fontsize=self.title_fontsize, pad=20)

        self._plot(
            ax=ax1,
            legend=("lower right", len(models)),
            ylabel="Residuals",
            xlabel="True value",
            xlim=xlim,
            ylim=ylim,
            figsize=figsize,
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_confusion_matrix(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        dataset: str = "test",
        normalize: bool = False,
        title: Optional[str] = None,
        figsize: Optional[Tuple[SCALAR, SCALAR]] = None,
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot a model's confusion matrix.

        Only for classification tasks.
        If 1 model: Plot the confusion matrix in a heatmap.
        If >1 models: Compare TP, FP, FN and TN in a barplot. Not
                      implemented for multiclass classification tasks.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        dataset: str, optional (default="test")
            Data set on which to calculate the confusion matrix.
            Options are "train" or "test".

        normalize: bool, optional (default=False)
           Whether to normalize the matrix. Only for the heatmap plot.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=None)
            Figure's size, format as (x, y). If None, adapts size to
            the plot's type.

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        check_goal(self, "plot_confusion_matrix", "classification")
        models = self._get_subclass(models)

        if self.task.startswith("multi") and len(models) > 1:
            raise NotImplementedError(
                "The plot_confusion_matrix method does not support the comparison"
                " of various models for multiclass classification tasks."
            )

        # Create dataframe to plot with barh if len(models) > 1
        df = pd.DataFrame(
            index=[
                "True negatives",
                "False positives",
                "False negatives",
                "True positives",
            ]
        )
        if title is None:
            title = "Confusion matrix"

        for m in models:
            cm = m.scoring("cm", dataset.lower())

            if len(models) == 1:  # Create matrix heatmap
                if normalize:
                    cm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]
                    if title == "Confusion matrix":
                        title = "Normalized confusion matrix"

                fig = self._get_figure()
                ax = fig.add_subplot(BasePlotter._fig.grid)
                im = ax.imshow(cm, interpolation="nearest", cmap=plt.get_cmap("Blues"))

                # Create an axes on the right side of ax. The under of cax will
                # be 5% of ax and the padding between cax and ax will be fixed
                # at 0.3 inch.
                divider = make_axes_locatable(ax)
                cax = divider.append_axes("right", size="5%", pad=0.3)
                cbar = ax.figure.colorbar(im, cax=cax)
                ax.set(
                    xticks=np.arange(cm.shape[1]),
                    yticks=np.arange(cm.shape[0]),
                    xticklabels=self.mapping.keys(),
                    yticklabels=self.mapping.keys(),
                )

                # Loop over data dimensions and create text annotations
                fmt = ".2f" if normalize else "d"
                for i in range(cm.shape[0]):
                    for j in range(cm.shape[1]):
                        ax.text(
                            x=j,
                            y=i,
                            s=format(cm[i, j], fmt),
                            ha="center",
                            va="center",
                            fontsize=self.tick_fontsize,
                            color="w" if cm[i, j] > cm.max() / 2.0 else "k",
                        )

                cbar.set_label(
                    label="Count",
                    fontsize=self.label_fontsize,
                    labelpad=15,
                    rotation=270,
                )
                cbar.ax.tick_params(labelsize=self.tick_fontsize)
                ax.grid(False)

            else:  # Create barplot
                df[m.name] = cm.ravel()

        if len(models) > 1:
            ax = df.plot.barh(width=0.6)
            self._plot(
                ax=ax,
                title=title,
                legend=("best", len(models)),
                xlabel="Count",
                figsize=figsize if figsize else (10, 6),
                filename=filename,
                display=display,
            )
        else:
            self._plot(
                ax=ax,
                title=title,
                xlabel="Predicted label",
                ylabel="True label",
                figsize=figsize if figsize else (8, 8),
                filename=filename,
                display=display
            )

    @composed(crash, plot_from_model, typechecked)
    def plot_threshold(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        metric: Optional[Union[str, callable, SEQUENCE_TYPES]] = None,
        dataset: str = "test",
        steps: int = 100,
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot metric performances against threshold values.

        Only for binary classification tasks.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        metric: str, callable, sequence or None, optional (default=None)
            Metric(s) to plot. These can be one of sklearn's SCORERS,
            a metric function or a scorer. If None, the metric used to
            run the pipeline is used.

        dataset: str, optional (default="test")
            Data set on which to calculate the metric. Options are
            "train", "test" or "both".

        steps: int, optional (default=100)
            Number of thresholds measured.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        check_binary_task(self, "plot_threshold")
        models = self._get_subclass(models)
        dataset = self._get_set(dataset)
        check_predict_proba(models, "plot_threshold")

        if metric is None:
            metric = self.metric_
        elif not isinstance(metric, list):
            metric = [metric]

        # Convert all strings to functions
        metric_list = []
        for met in metric:
            if isinstance(met, str):  # It is one of sklearn predefined metrics
                if met in METRIC_ACRONYMS:
                    met = METRIC_ACRONYMS[met]

                if met not in SCORERS:
                    raise ValueError(
                        "Unknown value for the metric parameter, "
                        f"got {met}. Try one of {list(SCORERS)}."
                    )
                metric_list.append(SCORERS[met]._score_func)
            elif hasattr(met, "_score_func"):  # It is a scorer
                metric_list.append(met._score_func)
            else:  # It is a metric function
                metric_list.append(met)

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        steps = np.linspace(0, 1, steps)
        for m in models:
            for met in metric_list:  # Create dict of empty arrays
                for set_ in dataset:
                    results = []
                    for step in steps:
                        predictions = (
                            getattr(m, f"predict_proba_{set_}")[:, 1] >= step
                        ).astype(bool)
                        results.append(met(getattr(m, f"y_{set_}"), predictions))

                    if len(models) == 1:
                        l_set = f"{set_} - " if len(dataset) > 1 else ""
                        label = f"{l_set}{met.__name__}"
                    else:
                        l_set = f" - {set_}" if len(dataset) > 1 else ""
                        label = f"{m.name}{l_set} ({met.__name__})"
                    ax.plot(steps, results, label=label, lw=2)

        self._plot(
            ax=ax,
            title="Performance vs threshold" if title is None else title,
            legend=("best", len(models)),
            xlabel="Threshold",
            ylabel="Score",
            figsize=figsize,
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_probabilities(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        dataset: str = "test",
        target: Union[int, str] = 1,
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the probability distribution of the target classes.

        Only for classification tasks.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        dataset: str, optional (default="test")
            Data set on which to calculate the metric. Options are
            "train", "test" or "both".

        target: int or str, optional (default=1)
            Probability of being that class in the target column as
            index or name. Only for multiclass classification.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        check_goal(self, "plot_probabilities", "classification")
        models = self._get_subclass(models)
        dataset = self._get_set(dataset)
        tgt_int, tgt_str = self._get_target(target)
        check_predict_proba(models, "plot_probabilities")

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        for m in models:
            for set_ in dataset:
                for key, value in self.mapping.items():
                    # Get indices per class
                    idx = np.where(getattr(m, f"y_{set_}") == value)[0]

                    if len(models) == 1:
                        l_set = f"{set_} - " if len(dataset) > 1 else ""
                        label = f"{l_set}Category:{key}"
                    else:
                        l_set = f" - {set_}" if len(dataset) > 1 else ""
                        label = f"{m.name}{l_set} (Category: {key})"
                    sns.kdeplot(
                        x=getattr(m, f"predict_proba_{set_}")[idx, tgt_int],
                        fill=True,
                        ax=ax,
                        label=label,
                    )

        if title is None:
            title = f"Probability distribution for {m.y.name}={tgt_str}"
        self._plot(
            ax=ax,
            title=title,
            legend=("best", len(models)),
            xlabel="Probability",
            ylabel="Count",
            xlim=(0, 1),
            figsize=figsize,
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_calibration(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        n_bins: int = 10,
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 10),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the calibration curve for a binary classifier.

        Well calibrated classifiers are probabilistic classifiers for
        which the output of the `predict_proba` method can be directly
        interpreted as a confidence level. For instance a well
        calibrated (binary) classifier should classify the samples such
        that among the samples to which it gave a `predict_proba` value
        close to 0.8, approx. 80% actually belong to the positive class.
        This figure shows two plots: the calibration curve, where the
        x-axis represents the average predicted probability in each bin
        and the y-axis is the fraction of positives, i.e. the proportion
        of samples whose class is the positive class (in each bin); and
        a distribution of all predicted probabilities of the classifier.
        Code snippets from https://scikit-learn.org/stable/auto_examples/
        calibration/plot_calibration_curve.html

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        n_bins: int, optional (default=10)
            Number of bins for the calibration calculation and the
            histogram. Minimum of 5 required.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 10))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        check_binary_task(self, "plot_calibration")
        models = self._get_subclass(models)

        if n_bins < 5:
            raise ValueError(
                "Invalid value for the n_bins parameter."
                f"Value should be >=5, got {n_bins}."
            )

        fig = self._get_figure()
        gs = GridSpecFromSubplotSpec(4, 1, BasePlotter._fig.grid, hspace=0.05)
        ax1 = fig.add_subplot(gs[:3, 0])
        ax2 = fig.add_subplot(gs[3:4, 0], sharex=ax1)
        ax1.plot([0, 1], [0, 1], color="k", ls="--")
        for m in models:
            if hasattr(m.estimator, "decision_function"):
                prob = m.decision_function_test
                prob = (prob - prob.min()) / (prob.max() - prob.min())
            elif hasattr(m.estimator, "predict_proba"):
                prob = m.predict_proba_test[:, 1]

            # Get calibration (frac of positives and predicted values)
            frac_pos, pred = calibration_curve(self.y_test, prob, n_bins=n_bins)

            ax1.plot(pred, frac_pos, marker="o", lw=2, label=f"{m.name}")
            ax2.hist(prob, n_bins, range=(0, 1), label=m.name, histtype="step", lw=2)

        plt.setp(ax1.get_xticklabels(), visible=False)
        self._plot(
            ax=ax1,
            title="Calibration curve" if title is None else title,
            legend=("lower right" if len(models) > 1 else False, len(models)),
            ylabel="Fraction of positives",
            ylim=(-0.05, 1.05),
        )
        self._plot(
            ax=ax2,
            xlabel="Predicted value",
            ylabel="Count",
            figsize=figsize,
            filename=filename,
            display=display
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_gains(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        dataset: str = "test",
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the cumulative gains curve.

        Only for binary classification tasks. Code snippet from
        https://github.com/reiinakano/scikit-plot/

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        dataset: str, optional (default="test")
            Data set on which to calculate the gains curve. Options
            are "train", "test" or "both".

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        check_binary_task(self, "plot_gains")
        models = self._get_subclass(models)
        dataset = self._get_set(dataset)
        check_predict_proba(models, "plot_gains")

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        ax.plot([0, 1], [0, 1], "k--", lw=2, alpha=0.7)
        for m in models:
            for set_ in dataset:
                y_true = getattr(m, f"y_{set_}") == 1  # Make y_true a bool vector

                # Get sorted indices and correct for the test set
                sort_idx = np.argsort(getattr(m, f"predict_proba_{set_}")[:, 1])[::-1]

                if set_ == "test":
                    sort_idx = [i + len(m.y_train) for i in sort_idx]

                # Compute cumulative gains
                gains = np.cumsum(y_true.loc[sort_idx]) / float(np.sum(y_true))

                x = np.arange(start=1, stop=len(y_true) + 1) / float(len(y_true))
                label = ""
                if len(models) > 1:
                    label += m.name
                if len(models) > 1 and len(dataset) > 1:
                    label += " - "
                if len(dataset) > 1:
                    label += set_
                ax.plot(x, gains, lw=2, label=label)

        self._plot(
            ax=ax,
            title="Cumulative gains curve" if title is None else title,
            legend=("lower right", len(models)),
            xlabel="Fraction of sample",
            ylabel="Gain",
            xlim=(0, 1),
            ylim=(0, 1.02),
            figsize=figsize,
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def plot_lift(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        dataset: str = "test",
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the lift curve.

        Only for binary classification tasks. Code snippet from
        https://github.com/reiinakano/scikit-plot/

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        dataset: str, optional (default="test")
            Data set on which to calculate the metric. Options are
            "train", "test" or "both".

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        check_binary_task(self, "plot_lift")

        models = self._get_subclass(models)
        dataset = self._get_set(dataset)
        check_predict_proba(models, "plot_lift")

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        ax.plot([0, 1], [1, 1], "k--", lw=2, alpha=0.7)
        for m in models:
            for set_ in dataset:
                y_true = getattr(m, f"y_{set_}") == 1  # Make y_true a bool vector

                # Get sorted indices and correct for the test set
                sort_idx = np.argsort(getattr(m, f"predict_proba_{set_}")[:, 1])[::-1]

                if set_ == "test":  # Add the training set length to the indices
                    sort_idx = [i + len(m.y_train) for i in sort_idx]

                # Compute cumulative gains
                gains = np.cumsum(y_true.loc[sort_idx]) / float(np.sum(y_true))

                if len(models) == 1:
                    l_set = f"{set_} - " if len(dataset) > 1 else ""
                    label = f"{l_set}Lift={round(m.scoring('lift'), 3)}"
                else:
                    l_set = f" - {set_}" if len(dataset) > 1 else ""
                    label = f"{m.name}{l_set} (Lift={round(m.scoring('lift'), 3)})"
                x = np.arange(start=1, stop=len(y_true) + 1) / float(len(y_true))
                ax.plot(x, gains / x, lw=2, label=label)

        self._plot(
            ax=ax,
            title="Lift curve" if title is None else title,
            legend=("upper right", len(models)),
            xlabel="Fraction of sample",
            ylabel="Lift",
            xlim=(0, 1),
            figsize=figsize,
            filename=filename,
            display=display,
        )

    # SHAP plots =================================================== >>

    @composed(crash, plot_from_model, typechecked)
    def force_plot(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        index: Optional[Union[int, tuple]] = None,
        target: Union[str, int] = 1,
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (14, 6),
        filename: Optional[str] = None,
        display: bool = True,
        **kwargs,
    ):
        """Plot SHAP's force plot.

        Visualize the given SHAP values with an additive force layout.
        The explainer will be chosen automatically based on the model's
        type. Note that by default this plot will render using
        javascript. For a regular figure use `matplotlib=True` (this
        option is only available when only 1 row is selected through
        the `index` parameter).

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected. Note that selecting multiple models
            will raise an exception. To avoid this, call the plot from
            a model.

        index: int, tuple or None, optional (default=None)
            Indices of the rows in the dataset to plot. If (n, m),
            select rows n until m. If None, select all rows in the
            test set.

        target: int or str, optional (default=1)
            Index or name of the class in the target column to look at.
            Only for multi-class classification tasks.

        title: str or None, optional (default=None)
            Plot's title. If None, the title is left empty.

        figsize: tuple, optional (default=(14, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If matplotlib=False, the figure will
            be saved as an html file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        **kwargs
            Additional keyword arguments for shap's force_plot.

        """
        check_dim(self, "force_plot")
        check_is_fitted(self, "results")

        if getattr(BasePlotter._fig, "is_canvas", None):
            raise PermissionError(
                "The force_plot method can not be called from a "
                "canvas because of incompatibility of the APIs."
            )

        m = self._get_subclass(models, max_one=True)
        rows = self._get_index(index, m)
        tgt_int, tgt_str = self._get_target(target)
        shap_values, expected_value = self._get_shap(m, rows, tgt_int)

        sns.set_style("white")  # Only for this plot
        plot = shap.force_plot(
            base_value=expected_value,
            shap_values=shap_values,
            features=rows,
            figsize=figsize,
            show=False,
            **kwargs,
        )

        sns.set_style(self.style)
        if kwargs.get("matplotlib"):
            self._plot(title, filename=filename, display=display)
        else:
            if filename:  # Save to an html file
                fn = filename if filename.endswith(".html") else filename + ".html"
                shap.save_html(fn, plot)
            if display:
                try:  # Render if possible (for notebooks)
                    from IPython.display import display

                    shap.initjs()
                    display(plot)
                except ModuleNotFoundError:
                    pass

    @composed(crash, plot_from_model, typechecked)
    def dependence_plot(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        index: Union[int, str] = "rank(1)",
        target: Union[int, str] = 1,
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
        **kwargs,
    ):
        """Plot SHAP's dependence plot.

        Plots the value of the feature on the x-axis and the SHAP value
        of the same feature on the y-axis. This shows how the model
        depends on the given feature, and is like a richer extension of
        the classical partial dependence plots. Vertical dispersion of
        the data points represents interaction effects. The explainer
        will be chosen automatically based on the model's type.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected. Note that selecting multiple models
            will raise an exception. To avoid this, call the plot from
            a model.

        index: int or str, optional (default="rank(1)")
            If int, it is the index of the feature to plot. If string
            it is either the name of the feature to plot, or it can
            have the form "rank(int)" to specify the feature with that
            rank (ordered by mean absolute SHAP value over all samples).

        target: int or str, optional (default=1)
            Index or name of the class in the target column to look at.
            Only for multi-class classification tasks.

        title: str or None, optional (default=None)
            Plot's title. If None, the title is left empty.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        **kwargs
            Additional keyword arguments for shap's dependence_plot.

        """
        check_dim(self, "dependence_plot")
        check_is_fitted(self, "results")

        m = self._get_subclass(models, max_one=True)
        tgt_int, tgt_str = self._get_target(target)
        shap_values, _ = self._get_shap(m, m.X_test, tgt_int)

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        shap.dependence_plot(
            ind=index,
            shap_values=shap_values,
            features=m.X_test,
            ax=ax,
            show=False,
            **kwargs,
        )

        ax.set_xlabel(ax.get_xlabel(), fontsize=self.label_fontsize, labelpad=12)
        ax.set_ylabel(ax.get_ylabel(), fontsize=self.label_fontsize, labelpad=12)
        if title is None:
            title = "Dependence plot"
            if tgt_int == 0 or self.task.startswith("multi"):
                title += f" for {m.y.name}={tgt_str}"
        self._plot(ax, title=title, figsize=figsize, filename=filename, display=display)

    @composed(crash, plot_from_model, typechecked)
    def summary_plot(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        show: Optional[int] = None,
        target: Union[int, str] = 1,
        title: Optional[str] = None,
        figsize: Optional[Tuple[SCALAR, SCALAR]] = None,
        filename: Optional[str] = None,
        display: bool = True,
        **kwargs,
    ):
        """Plot SHAP's summary plot.

        Create a SHAP beeswarm plot, colored by feature values when
        they are provided. The explainer will be chosen automatically
        based on the model's type.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected. Note that selecting multiple
            models will raise an exception. To avoid this, call
            the plot from a model.

        show: int or None, optional (default=None)
            Number of features to show in the plot. None to show all.

        target: int or str, optional (default=1)
            Index or name of the class in the target column to look at.
            Only for multi-class classification tasks.

        title: str or None, optional (default=None)
            Plot's title. If None, the title is left empty.

        figsize: tuple, optional (default=None)
            Figure's size, format as (x, y). If None, adapts size
            to the`show` parameter.

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        **kwargs
            Additional keyword arguments for shap's summary_plot.

        """
        check_dim(self, "summary_plot")
        check_is_fitted(self, "results")

        m = self._get_subclass(models, max_one=True)
        show = self._get_show(show)
        tgt_int, tgt_str = self._get_target(target)
        shap_values, _ = self._get_shap(m, m.X_test, tgt_int)

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        shap.summary_plot(
            shap_values=shap_values,
            features=m.X_test,
            max_display=show,
            plot_size=figsize,
            show=False,
            **kwargs,
        )

        ax.set_xlabel(ax.get_xlabel(), fontsize=self.label_fontsize, labelpad=12)
        if title is None:
            title = "Summary plot"
            if tgt_int == 0 or self.task.startswith("multi"):
                title += f" for {m.y.name}={tgt_str}"
        self._plot(
            ax=ax,
            title=title,
            figsize=figsize if figsize else (10, 4 + show // 2),
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def decision_plot(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        index: Optional[Union[int, tuple]] = None,
        show: Optional[int] = None,
        target: Union[int, str] = 1,
        title: Optional[str] = None,
        figsize: Optional[Tuple[SCALAR, SCALAR]] = None,
        filename: Optional[str] = None,
        display: bool = True,
        **kwargs,
    ):
        """Plot SHAP's decision plot.

        Visualize model decisions using cumulative SHAP values. Each
        plotted line explains a single model prediction. If a single
        prediction is plotted, feature values will be printed in the
        plot (if supplied). If multiple predictions are plotted
        together, feature values will not be printed. Plotting too
        many predictions together will make the plot unintelligible.
        The explainer will be chosen automatically based on the model's
        type.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected. Note that selecting multiple
            models will raise an exception. To avoid this, call
            the plot from a model.

        index: int, tuple or None, optional (default=None)
            Indices of the rows in the dataset to plot. If shape
            (n, m), select rows n until m. If None, select all
            rows in the test set.

        show: int or None, optional (default=None)
            Number of features (ordered by importance) to show in
            the plot. None to show all.

        target: int or str, optional (default=1)
            Index or name of the class in the target column to look at.
            Only for multi-class classification tasks.

        title: str or None, optional (default=None)
            Plot's title. If None, the title is left empty.

        figsize: tuple, optional (default=None)
            Figure's size, format as (x, y). If None, adapts size
            to the number of features.

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        **kwargs
            Additional keyword arguments for shap's decision_plot.

        """
        check_dim(self, "decision_plot")
        check_is_fitted(self, "results")

        m = self._get_subclass(models, max_one=True)
        rows = self._get_index(index, m)
        show = self._get_show(show)
        tgt_int, tgt_str = self._get_target(target)
        shap_values, expected_value = self._get_shap(m, rows, tgt_int)

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        shap.decision_plot(
            base_value=expected_value,
            shap_values=shap_values,
            features=rows,
            feature_display_range=slice(-1, -show - 1, -1),
            auto_size_plot=False,
            show=False,
            **kwargs,
        )

        ax.set_xlabel(ax.get_xlabel(), fontsize=self.label_fontsize, labelpad=12)
        if title is None:
            title = "Decision plot"
            if tgt_int == 0 or self.task.startswith("multi"):
                title += f" for {m.y.name}={tgt_str}"
        self._plot(
            ax=ax,
            title=title,
            figsize=figsize if figsize else (10, 4 + show // 2),
            filename=filename,
            display=display,
        )

    @composed(crash, plot_from_model, typechecked)
    def waterfall_plot(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        index: Optional[int] = None,
        show: Optional[int] = None,
        target: Union[int, str] = 1,
        title: Optional[str] = None,
        figsize: Optional[Tuple[SCALAR, SCALAR]] = None,
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot SHAP's waterfall plot for a single prediction.

        The SHAP value of a feature represents the impact of the
        evidence provided by that feature on the model’s output. The
        waterfall plot is designed to visually display how the SHAP
        values (evidence) of each feature move the model output from
        our prior expectation under the background data distribution,
        to the final model prediction given the evidence of all the
        features. Features are sorted by the magnitude of their SHAP
        values with the smallest magnitude features grouped together
        at the bottom of the plot when the number of features in the
        models exceeds the `show` parameter.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected. Note that selecting multiple
            models will raise an exception. To avoid this, call
            the plot from a model.

        index: int or None, optional (default=None)
            Index of the row in the dataset to plot. If None,
            selects a random row in the test set.

        show: int or None, optional (default=None)
            Number of features (ordered by importance) to show in
            the plot. None to show all.

        target: int or str, optional (default=1)
            Index or name of the class in the target column to look at.
            Only for multi-class classification tasks.

        title: str or None, optional (default=None)
            Plot's title. If None, the title is left empty.

        figsize: tuple, optional (default=None)
            Figure's size, format as (x, y). If None, adapts size to
            the number of features.

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_dim(self, "waterfall_plot")
        check_is_fitted(self, "results")

        m = self._get_subclass(models, max_one=True)
        # If index is None, give a random index of the test set
        if index is None:
            index = random.randint(len(m.train), len(m.dataset))

        rows = self._get_index(index, m)
        show = self._get_show(show)
        tgt_int, tgt_str = self._get_target(target)
        shap_values, expected_value = self._get_shap(m, rows, tgt_int)

        # Use legacy for consistency with other plot methods
        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        shap.plots._waterfall.waterfall_legacy(
            expected_value=expected_value,
            shap_values=shap_values[0],
            features=rows.iloc[0],
            max_display=show,
            show=False,
        )

        if title is None:
            title = "Waterfall plot"
            if tgt_int == 0 or self.task.startswith("multi"):
                title += f" for {m.y.name}={tgt_str}"
        self._plot(
            ax=ax,
            title=title,
            figsize=figsize if figsize else (10, 4 + show // 2),
            filename=filename,
            display=display
        )


class SuccessiveHalvingPlotter(BaseModelPlotter):
    """Plots for the SuccessiveHalving classes."""

    @composed(crash, plot_from_model, typechecked)
    def plot_successive_halving(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        metric: Union[int, str] = 0,
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot scores per iteration of the successive halving.

        Only available if the models were fitted via successive_halving.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        metric: int or str, optional (default=0)
            Index or name of the metric. Only for multi-metric runs.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        models = self._get_subclass(models)
        metric = self._get_metric(metric)

        # Either it's a SuccessiveHalving class or atom with a sh approach
        condition_1 = self.__class__.__name__.startswith("SuccessiveHalving")
        condition_2 = getattr(self, "_approach", "") == "SuccessiveHalving"
        if not condition_1 and not condition_2:
            raise PermissionError(
                "You need to run the pipeline using successive "
                "halving to use the plot_successive_halving method!"
            )

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)

        x, y, std = defaultdict(list), defaultdict(list), defaultdict(list)
        for m in models:
            y[m.name[:-1]].append(get_best_score(m, metric))
            x[m.name[:-1]].append(len(self.train) // m._train_idx)
            if m.std_bagging:
                std[m.name[:-1]].append(lst(m.std_bagging)[metric])

        for k in x:
            if not std:
                ax.plot(x[k], y[k], lw=2, marker="o", label=k)
            else:
                ax.plot(x[k], y[k], lw=2, marker="o")
                ax.errorbar(x[k], y[k], std[k], lw=1, marker="o", label=k)
                plus, minus = np.add(y[k], std[k]), np.subtract(y[k], std[k])
                ax.fill_between(x[k], plus, minus, alpha=0.3)

        range_ = self._results.index.get_level_values("n_models")
        ax.set_xlim(max(range_) + 0.1, min(range_) - 0.1)
        ax.set_xticks(range(1, max(range_) + 1))
        self._plot(
            ax=ax,
            title="Successive halving results" if title is None else title,
            legend=("lower right", len(x)),
            xlabel="n_models",
            ylabel=self.metric_[metric].name,
            figsize=figsize,
            filename=filename,
            display=display,
        )


class TrainSizingPlotter(BaseModelPlotter):
    """Plots for the TrainSizing classes."""

    @composed(crash, plot_from_model, typechecked)
    def plot_learning_curve(
        self,
        models: Optional[Union[str, SEQUENCE_TYPES]] = None,
        metric: Union[int, str] = 0,
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (10, 6),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the learning curve: score vs number of training samples.

        Only available if the models were fitted using train sizing.

        Parameters
        ----------
        models: str, sequence or None, optional (default=None)
            Name of the models to plot. If None, all models in the
            pipeline are selected.

        metric: int or str, optional (default=0)
            Index or name of the metric. Only for multi-metric runs.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple, optional (default=(10, 6))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        check_is_fitted(self, "results")
        models = self._get_subclass(models)
        metric = self._get_metric(metric)

        # Either it's a TrainSizing class or atom with a ts approach
        condition_1 = self.__class__.__name__.startswith("TrainSizing")
        condition_2 = getattr(self, "_approach", "") == "TrainSizing"
        if not condition_1 and not condition_2:
            raise PermissionError(
                "You need to run the pipeline using train "
                "sizing to use the plot_learning_curve method!"
            )

        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)

        x, y, std = defaultdict(list), defaultdict(list), defaultdict(list)
        for m in models:
            y[m.name[:-1]].append(get_best_score(m, metric))
            x[m.name[:-1]].append(m._train_idx)
            if m.std_bagging:
                std[m.name[:-1]].append(lst(m.std_bagging)[metric])

        for k in x:
            if not std:
                ax.plot(x[k], y[k], lw=2, marker="o", label=k)
            else:
                ax.plot(x[k], y[k], lw=2, marker="o")
                ax.errorbar(x[k], y[k], std[k], lw=1, marker="o", label=k)
                plus, minus = np.add(y[k], std[k]), np.subtract(y[k], std[k])
                ax.fill_between(x[k], plus, minus, alpha=0.3)

        ax.ticklabel_format(axis="x", style="sci", scilimits=(0, 4))
        self._plot(
            ax=ax,
            title="Learning curve" if title is None else title,
            legend=("lower right", len(x)),
            xlabel="Number of training samples",
            ylabel=self.metric_[metric].name,
            figsize=figsize,
            filename=filename,
            display=display,
        )


class ATOMPlotter(FSPlotter, SuccessiveHalvingPlotter, TrainSizingPlotter):
    """Plots for the ATOM class."""

    @composed(crash, typechecked)
    def plot_correlation(
        self,
        method: str = "pearson",
        title: Optional[str] = None,
        figsize: Tuple[SCALAR, SCALAR] = (8, 7),
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot the correlation matrix. Ignores non-numeric columns.

        Parameters
        ----------
        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        method: str, optional (default="pearson")
            Method of correlation. Choose from "pearson", "kendall"
            or "spearman".

        figsize: tuple, optional (default=(8, 7))
            Figure's size, format as (x, y).

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        # Compute the correlation matrix
        corr = self.dataset.corr(method=method)

        # Drop first row and last column (diagonal line)
        corr = corr.iloc[1:].drop(self.target, axis=1)

        # Generate a mask for the upper triangle
        # k=1 means keep outermost diagonal line
        mask = np.zeros_like(corr, dtype=np.bool)
        mask[np.triu_indices_from(mask, k=1)] = True

        sns.set_style("white")  # Only for this plot
        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)
        sns.heatmap(
            data=corr,
            mask=mask,
            cmap=sns.diverging_palette(220, 10, as_cmap=True),
            vmax=0.3,
            center=0,
            linewidths=0.5,
            ax=ax,
            cbar_kws={"shrink": 0.8},
        )
        sns.set_style(self.style)  # Set back to original style
        self._plot(
            ax=ax,
            title="Feature correlation matrix" if title is None else title,
            figsize=figsize,
            filename=filename,
            display=display,
        )

    @composed(crash, typechecked)
    def plot_pipeline(
        self,
        show_params: bool = True,
        branch: Optional[str] = None,
        title: Optional[str] = None,
        figsize: Optional[Tuple[SCALAR, SCALAR]] = None,
        filename: Optional[str] = None,
        display: bool = True,
    ):
        """Plot a diagram of every estimator in a branch.

        Parameters
        ----------
        show_params: bool, optional (default=True)
            Whether to show the parameters used for every estimator.

        branch: str or None, optional (default=None)
            Name of the branch to plot. If None, plot the current
            active branch.

        title: str or None, optional (default=None)
            Plot's title. If None, the default option is used.

        figsize: tuple or None, optional (default=None)
            Figure's size, format as (x, y). If None, adapts size to
            the length of the pipeline.

        filename: str or None, optional (default=None)
            Name of the file. If None, the figure is not saved.

        display: bool, optional (default=True)
            Whether to render the plot.

        """
        if not branch:
            branch = self._current
        elif branch not in self._branches:
            raise ValueError(
                "Invalid value for the branch parameter. Unknown branch,"
                f" got {branch}. Choose from: {', '.join(self._branches)}."
            )

        # Calculate figure's limits
        params = []
        ylim = 30
        for est in self._branches[branch].pipeline:
            ylim += 15
            if show_params:
                params.append([
                    p for p in signature(est.__init__).parameters
                    if p not in BaseTransformer.attrs + ["self"]
                ])
                ylim += len(params[-1]) * 10

        sns.set_style("white")  # Only for this plot
        fig = self._get_figure()
        ax = fig.add_subplot(BasePlotter._fig.grid)

        # Shared parameters for the blocks
        con = ConnectionStyle("angle", angleA=0, angleB=90, rad=0)
        arrow = dict(arrowstyle="<|-", lw=1, color="k", connectionstyle=con)

        # Draw the main class
        ax.text(
            x=20,
            y=ylim - 20,
            s=self.__class__.__name__,
            ha="center",
            size=self.label_fontsize + 2,
        )

        pos_param = ylim - 20
        pos_estimator = pos_param

        for i, est in enumerate(self._branches[branch].pipeline):
            ax.annotate(
                text=est.__class__.__name__,
                xy=(15, pos_estimator),
                xytext=(30, pos_param - 3 - 15),
                ha="left",
                size=self.label_fontsize,
                arrowprops=arrow,
            )

            pos_param -= 15
            pos_estimator = pos_param

            if show_params:
                for j, key in enumerate(params[i]):
                    ax.annotate(
                        text=key + ": " + str(flt(getattr(est, key))),
                        xy=(32, pos_param - 6 if j == 0 else pos_param + 1),
                        xytext=(40, pos_param - 12),
                        ha="left",
                        size=self.label_fontsize - 4,
                        arrowprops=arrow,
                    )

                    pos_param -= 10

        ax.axes.get_xaxis().set_ticks([])
        ax.axes.get_yaxis().set_ticks([])
        plt.axis("off")

        sns.set_style(self.style)  # Set back to original style
        self._plot(
            ax=ax,
            title=title,
            xlim=(0, 100),
            ylim=(0, ylim),
            figsize=figsize if figsize else (8, ylim // 30),
            filename=filename,
            display=display,
        )
